{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a href=\"https://colab.research.google.com/github/JasonGross/neural-net-coq-interp/blob/main/writeups/Formalizing_Transformers_For_Mech_Interp_Folks_Max_of_small_n_Jason_Gross%2C_Thomas_Kwa%2C_Rajashree_Agrwal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formalizing Transformers, For Mech Interp Folks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [*Towards Monosemanticity: Decomposing Language Models With Dictionary Learning*](https://transformer-circuits.pub/2023/monosemantic-features), Bricken et al. say:\n",
    "> #### How can we tell if the autoencoder is working?\n",
    ">\n",
    "> Usually in machine learning we can quite easily tell if a method is working by looking at an easily-measured quantity like the test loss. We spent quite some time searching for an equivalent metric to guide our efforts here, and unfortunately have yet to find anything satisfactory.\n",
    ">\n",
    "> We began by looking for an information-based metric, so that we could say in some sense that the best factorization is the one that minimizes the total information of the autoencoder and the data. Unfortunately, this total information did not generally correlate with subjective feature interpretability or activation sparsity. (Runs whose feature activations had an average L0 norm in the hundreds but low reconstruction error could have lower total information than those with smaller average L0 norm and higher reconstruction error.)\n",
    ">\n",
    "> Thus we ended up using a combination of several additional metrics to guide our investigations:\n",
    ">\n",
    "> 1. **Manual inspection:** Do the features seem interpretable?\n",
    "> 2. **Feature density:** we found that the number of “live” features and the percentage of tokens on which they fire to be an extremely useful guide. (See appendix for details.)\n",
    "> 3. **Reconstruction loss:** How well does the autoencoder reconstruct the MLP activations? Our goal is ultimately to explain the function of the MLP layer, so the MSE loss should be low.\n",
    "> 4. **Toy models:** Having toy models where we know the ground truth and so can cleanly evaluate the autoencoder’s performance was crucial to our early progress.\n",
    ">\n",
    ">Interpreting or measuring some of these signals can be difficult, though. For instance, at various points we thought we saw features which at first didn’t make any sense, but with deeper inspection we could understand. Likewise, while we have identified some desiderata for the distribution of feature densities, there is much that we still do not understand and which prevents this from providing a clear signal of progress.\n",
    ">\n",
    ">We think it would be very helpful if we could identify better metrics for dictionary learning solutions from sparse autoencoders trained on transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This write-up is, in some sense, a response to the above quote.\n",
    "\n",
    "**Vision:** We want to be able to automate the discovery of mechanistic interpretation of neural networks.\n",
    "\n",
    "There are two big questions:\n",
    "1. How do we discover the interpretation? (Answer: Engineering!)\n",
    "2. How do we ensure that the interpretation is good (correct, accurate, human-understandable, etc.)?\n",
    "\n",
    "We provide a case-study-based plausible theoretical grounding for answering the second question.\n",
    "\n",
    "Claim: Minimizing information *does* lead to human interpretability, *if* we are clear on what we are proposing to explain.\n",
    "\n",
    "There are three problems with [the information-based metric](https://transformer-circuits.pub/2023/may-update/index.html#simple-factorization) proposed by the authors of *Towards Monosemanticity*, but none of them are problems with the idea of minimizing information.\n",
    "\n",
    "*Towards Monosemanticity*, as we understand it, first specifies a problem to be solved and a language of solutions (\"reconstruct the matrix of activations $A \\approx SD$ where $S$ is sparse and $D$ is a dictionary of features\"); then specifies tunable hyperparameters (the coefficient of the sparsity penalty and the size of the dictionary); then finds for each hyperparameter setting the solution that minimizes loss (as measured by combining the reconstruction error and the sparsity penalty); and finally proposes a metric (total information) for comparing the solutions for different hyperparameter settings.\n",
    "\n",
    "1. Claim: We actually want a tunable knob for picking out interpretations on the pareto-frontier of complicatedness of interpretation (total information) vs. accuracy of interpretation (reconstruction error).  We don't expect there to be a single \"best\" interpretation in general, but instead a spectrum of interpretations that trade off accuracy for simplicity.  It might make sense to analyze the curve here, perhaps looking for inflection points (intuitively, places where a little more complexity buys a lot more accuracy, or conversely a small relaxation in accuracy saves a lot of complexity), but we shouldn't expect information-based metrics to pick out a single \"best interpretation\" for us.\n",
    "2. The correct trade-off to be looking at is either \"for a given bound on information, what's the explanation that provides the most accuracy\" or \"for a given bound on reconstruction loss, what's the most compact explanation\".  The procedure described above does not do this.  Both hyperparameters already control trade-offs between reconstruction loss vs two different proxies for compactness / information / interpretability, and then the resulting optimal solutions are compared using a third proxy.  We have no reason to expect sensible behavior from this procedure.\n",
    "3. We can try to measure the understandability / compactness of four different things:\n",
    "   1. Local behavioral description: The description of *what* the MLP is doing\n",
    "   2. Global behavioral description: The description of *what* the entire neural net is doing\n",
    "   3. Global behavioral expectation: The description of *how* it is that *what* the neural net is doing results in our desired behavior (e.g., low loss on the training distribution)\n",
    "   4. Global behavioral guarantee: The description of *how* it is that, when accounting for *everything* the neural net is doing, the neural net achieves our desired behavior.  This is (3), but also accounting for all of the \"boring\" things, e.g., this includes explanations of how it comes to be that the \"random noise\" from \"unimportant heads\" is small enough that it doesn't destroy the \"important signal\".\n",
    "\n",
    "Total information of the sparse autoencoder decomposition is a measurement of (1).\n",
    "There's no reason to expect that the directions in activation space should have a particularly compact representation (and most \"human interpretations\" just say \"there is a direction\" for a feature, not what the direction is), which, we believe, is why sparsity seems a better proxy for human interpretability in sparse autoencoders than total information.\n",
    "\n",
    "This is a problem when trying to minimize information metrics of (1), but *not* when correctly minimizing information metrics for (3) or (4) (and possibly not even for (2)).\n",
    "\n",
    "There are two things (TODO: replace \"things\") that ameliorate this issue:\n",
    "1. When minimizing information for (3) and (4), we expect the largest gains (complexity reductions) to come not from compact descriptions of the input distribution factoring, but in the ways a given factoring allows a computational complexity reduction.  The *reason* a feature is useful is that you can compute a desired property more simply using that feature than without it.  The benefit of a \"DNA feature\" is not just in having a compact description of when something is DNA and what should be predicted when something is DNA, but in that other computations can be more simply described when conditioned on DNA.  Or to take a simpler example, the benefit of having a \"size\" direction that is the principle component of the QK circuit in max-of-n is that it allows us to reason about the OV behavior *as a function of how much attention is paid to the largest element* ($\\mathcal{O}(\\text{d\\_vocab})$ possibilities) rather than as a function of all the sequences of attention weights ($\\mathcal{O}(\\text{d\\_vocab}^{\\text{n\\_ctx}})$ possibilities).  Next to this asymptotic reduction, the compactness of the description of the particular size direction is peanuts.\n",
    "2. If we pick a measure of information [that does not peanalize us for arbitrary choices](https://www.lesswrong.com/posts/KcvJXhKqx4itFNWty/k-complexity-is-silly-use-cross-entropy-instead), such as cross entropy, then we can can do even better!  If a choice is arbitrary (such as the image of the size direction under embedding followed by the query matrix), we won't get docked for the size of that description.  If a choice is not arbitrary, then there's something interesting going on, and we shouldn't be excluding it from our interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proofs and Heursitic Arguments from the Mech Interp Lens\n",
    "Proofs give a guarantee not just of the biggest thing that happens, but also how it comes to be that nothing else of interest is happening.  Heuristic arguments promise to solve the problem of how to (rigorously) separate out the “nothing interesting is going on” “default assumption” so we can measure its complexity separately, and find actually compact arguments of just the “interesting” “interpretable” stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would love to be able to claim that compactness of {proof, heuristic arg} is a good evaluation metric for human interpretability.  We don’t have nearly enough evidence for this, alas (future work!), so instead we aim to present a case-study as evidence that compactness for human interp has *firm formal grounding*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Claim That Needs A Better Section Heading\n",
    "We probably ultimately want a measurement of (2) \"the thing the neural net is doing\", as compactly as possible.\n",
    "But if you look closely, we don't have a direct way to measure this.  Instead we're saying \"what's a compact way of computing the same thing the neural net computes\".  But \"computing the thing the neural net computes\" *is not the same* as \"the computation the neural net is running\"!\n",
    "\n",
    "As far as we can tell, the current approaches try to proxy the gap with reconstruction loss on the quirks and errors made by the model.\n",
    "There's some value to this (\"there's many ways for relationships to fail, but the only way for them to succeed is respect\"), but it clearly isn't adequate for perfect models implementing distinct algorithms.\n",
    "And it feels bad to me, saying that we're relying on networks being quirky and error-prone to get decently accuracte explanation evaluations.\n",
    "\n",
    "We want to claim that a better proxy for the gap between \"computing the same thing\" and \"the computation that's being run\" is a *guarantee* (either a proof or a heuristic argument) that the particular computation being run computes the desired result.\n",
    "We'll pay some cost by using this as our proxy (we'll have no choice but to include the complete description of the computation being run, though hopefully we can avoid being bitten by this by using cross-entropy), but we believe this proxy gives much firmer grounding for evaluating mech interp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document, we'll walk through a small case-study or two, applying this frame of proofs and guarantees for mech interp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof Strategy\n",
    "The strategy we use for making guarantees about neural nets $NN$ is as follows:\n",
    "1. Fix an input distribution $D_I$ (e.g., the uniform distribution of sequences of length $n$ over a vocabulary of size $d$).  This defines the domain of discourse.\n",
    "2. Define a property $P$ to be established on a metric $M$ that can be calculated over the input-output pairs of $NN$ on this domain.  For example, $M$ might be accuracy, log-loss, etc, and $P$ might be $\\leq 0.01$ or $\\geq 99\\%$ or \"within $\\varepsilon$ of 0.8\".  Importantly, this theorem $P(M(NN(D_I)))$ could in theory be proven (or disproven) simply by computation, if we were willing to wait long enough.\n",
    "3. Argue that the property holds of the given $NN$’s computation, by:\n",
    "   - Finding/constructing a cheaper computation $C$ (taking as input the $NN$ weights & biases) such that\n",
    "     1. Establishing the property $P$ on this computation $C$ implies that $P$ holds on $M(NN(D_I))$ (symbolically: $P(C) \\Longrightarrow P(M(NN(D_I)))$)\n",
    "         - We're working on formalizing such proofs in the proof assistant Coq, but for this document we give our proofs as English arguments.\n",
    "     2. Establishing $P$ on $C$ is computationally feasible and straightforward\n",
    "\n",
    "Note: Compactness is important in each of these steps, but gives different things:\n",
    "- domain of discourse: compactness allows us to speak at all\n",
    "- property description: compactness gives understanding of *what outcomes happens*\n",
    "- description of computation $C$: compactness gives understanding of *why outcome happens*\n",
    "- cost of running $C$: compactness gives goodness of description / understanding of *how it comes to be that the model is implementing our described algorithm*\n",
    "- size of proof that $P(C) \\Longrightarrow P(M(NN(D_I))))$: compactness gives understanding of *why outcome happens **on NN***\n",
    "\n",
    "TODO: move the below somewhere else, or to a document for heursitic argument folks\n",
    "\n",
    "Aside: As we understand it, heuristic arguments also fit into this frame, as follows:\n",
    "1. Same (input distribution $D_I$)\n",
    "2. Usually the metric $M$ is not quite what’s described, instead there’s a standard metric $M'$ (like log-loss), and the actual metric, currently implicit and nebulous, maybe goes something like $\\mathbb{E}_{NN\\leftarrow D_{NN}}\\left[M'(NN(D_I))\\right]$ (where $\\mathbb{E}_{NN\\leftarrow D_{NN}}$ is \"the expected value over $NN$ drawn from a distribution of weights and biases $D_{NN}$\"), and then the property $P$ is the same\n",
    "3. Sorta same\n",
    "   - Estimator $G$ is a class of computations parameterized over heuristic args $\\pi$ such that:\n",
    "     1. We can establish in general that (under some restrictions), there exists / forall / for many $\\pi$s, with $C = G(NN|\\pi\\text{s})$, $P(C)$ implies $P(M(NN(D_I)))$ (plus some other properties that make it reasonable to have this only for some $\\pi$s)\n",
    "     2. Same, but even more so (establishing $P(C)$ should be ~linear in NN size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup: Max of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be looking at the problem of computing the max of two numbers.  We use a 1L attention-only transformer with vocab size 64, model size 32, no layer norm, no biases.  The input is a sequence of two (or later $n$) numbers, and we train on the cross-entropy loss of the prediction in the final sequence position and the correct maximum.  The model has been adversarially overtrained to the point where the accuracy is 100% and the loss is dominated by 32-bit floating point error in the final log-softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "High-level: The model pays more attention to larger numbers, and copies whatever it's paying attention to.\n",
    "\n",
    "More detail:\n",
    "- There is a \"size direction\" and a \"query direction\".  Tokens are embededded with more-or-less uniform overlap with the query direction and more-or-less monotonically-increasing overlap with the size direction (the curve seems to be cubic or quintic, for unclear reasons).\n",
    "- The QK circuit has extremely high overlap between the query-direction on the query side and the size-direction on the key side, so that the pre-softmax attention is essentially a scalar multiple of the overlap between the one-hot token vector and the size direction.  Everything else the QK circuit does is essentially negligable.\n",
    "- The OV circuit is a low-rank representation of a matrix with high (and more-or-less uniform) diagonal entries and low off-diagonal entries.  We have no explanation for how this comes to be the case.\n",
    "- (TODO: check this) There's some additional structure in the noise: query tokens with less overlap with the query direction have (a) less skip-connection noise, (b) larger gaps between the diagonal and off-diagonal entries in the OV circuit, and (c) smaller errors in size-direction overlap.  That is, the errors conspire in our benefit: query tokens that are worse for paying attention to larger tokens have correspondingly larger gaps between them and adjacent tokens in the size-direction, so that we still succeed in paying more attention to tokens larger than the query and less attention to tokens smaller than the query, and the copying behavior on small-gap sequences lines up for reasons we have not yet understood (merely verified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title setup\n",
    "# Is this the development version?\n",
    "DEV = True #@param {type:\"boolean\"}\n",
    "\n",
    "try:\n",
    "    import google.colab # type: ignore\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install packages\n",
    "    %pip install einops\n",
    "    %pip install jaxtyping\n",
    "    %pip install transformer_lens\n",
    "\n",
    "    # Code to download the necessary files (e.g. solutions, test funcs)\n",
    "    import os, sys\n",
    "    if not os.path.exists(\"utils\"):\n",
    "        !curl -o /content/main.zip https://codeload.github.com/JasonGross/neural-net-coq-interp/zip/refs/heads/main\n",
    "        !unzip /content/main.zip 'neural-net-coq-interp/training/*'\n",
    "        sys.path.append(\"/content/utils\")\n",
    "        os.remove(\"/content/main.zip\")\n",
    "        os.rename(\"neural-net-coq-interp/training\", \"utils\")\n",
    "        os.rmdir(\"neural-net-coq-interp\")\n",
    "else:\n",
    "    from IPython import get_ipython\n",
    "    ipython = get_ipython()\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "    import os, sys\n",
    "    if DEV:\n",
    "        sys.path.append(f\"{os.getcwd()}/../training\")\n",
    "    elif not os.path.exists(\"utils\"):\n",
    "        !curl -o /content/main.zip https://codeload.github.com/JasonGross/neural-net-coq-interp/zip/refs/heads/main\n",
    "        !unzip /content/main.zip 'neural-net-coq-interp/training/*'\n",
    "        sys.path.append(f\"{os.getcwd()}/utils\")\n",
    "        os.remove(\"/content/main.zip\")\n",
    "        os.rename(\"neural-net-coq-interp/training\", \"utils\")\n",
    "        os.rmdir(\"neural-net-coq-interp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title training_utils\n",
    "# # Data Generation\n",
    "\n",
    "# Helper functions for generating data and splitting it into batches for training.\n",
    "\n",
    "import datetime\n",
    "import os, os.path\n",
    "from pathlib import Path\n",
    "from typing import List, Any, Iterable, Optional\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import itertools\n",
    "\n",
    "import wandb\n",
    "\n",
    "# In[ ]:\n",
    "def default_device(deterministic: bool = False) -> str:\n",
    "   return \"cuda\" if torch.cuda.is_available() and not deterministic else \"cpu\"\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "DEFAULT_WANDB_ENTITY = 'team-jason' # 'tkwa-team' # 'team-jason'\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "def in_colab() -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if running in Google Colab, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "def get_pth_base_path(save_in_google_drive: bool = False, create: bool = True) -> Path:\n",
    "    \"\"\"\n",
    "    Returns the base path for saving models. If `save_in_google_drive` is True, returns the path to the Google Drive\n",
    "    folder where models are saved. Otherwise, returns the path to the local folder where models are saved.\n",
    "    \"\"\"\n",
    "    if in_colab():\n",
    "        if save_in_google_drive:\n",
    "            from google.colab import drive\n",
    "            drive.mount('/content/drive/')\n",
    "            pth_base_path = Path('/content/drive/MyDrive/Colab Notebooks/')\n",
    "        else:\n",
    "            pth_base_path = Path(\"/workspace/_scratch/\")\n",
    "    else:\n",
    "        pth_base_path = Path(os.getcwd())\n",
    "\n",
    "    pth_base_path = pth_base_path / 'trained-models'\n",
    "\n",
    "    if create and not os.path.exists(pth_base_path):\n",
    "        os.makedirs(pth_base_path)\n",
    "\n",
    "    return pth_base_path\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "def generate_all_sequences(n_digits: int, sequence_length: int = 2):\n",
    "  data = list(itertools.product(range(n_digits), repeat=sequence_length))\n",
    "  data = torch.tensor(data)\n",
    "  return data\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "def compute_all_tokens(model: HookedTransformer):\n",
    "    return generate_all_sequences(n_digits=model.cfg.d_vocab, sequence_length=model.cfg.n_ctx)\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "def shuffle_data(data):\n",
    "  indices = np.array(range(len(data)))\n",
    "  np.random.shuffle(indices)\n",
    "  data = data[indices]\n",
    "  return data\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "def make_testset_trainset(\n",
    "    model: HookedTransformer,\n",
    "    training_ratio=0.7,\n",
    "    force_adjacent=False):\n",
    "  \"\"\"\n",
    "  Generate a train and test set of tuples containing `sequence_length` integers with values 0 <= n < n_digits.\n",
    "\n",
    "  Args:\n",
    "      sequence_length (int): The length of each tuple in the dataset.\n",
    "      n_digits (int): The number of possible values for each element in the tuple.\n",
    "      training_ratio (float): The ratio of the size of the training set to the full dataset.\n",
    "      force_adjacent (bool): Whether to make training adversarial (force to include all (x, x +- 1))\n",
    "\n",
    "  Returns:\n",
    "      Tuple[List[Tuple[int, ...]], List[Tuple[int, ...]]]: A tuple containing the training set and test set.\n",
    "          The training set contains `training_ratio` percent of the full dataset, while the test set contains the\n",
    "          remaining data. Each set is a list of tuples containing `sequence_length` integers with values 0 <= n < n_digits.\n",
    "          The tuples have been shuffled before being split into the train and test sets.\n",
    "  \"\"\"\n",
    "  data = compute_all_tokens(model)\n",
    "\n",
    "  data = shuffle_data(data)\n",
    "\n",
    "  if force_adjacent:\n",
    "    idxs = (data[:,0] - data[:,1]).abs() == 1\n",
    "    data, extra_data = data[~idxs], data[idxs]\n",
    "    data = torch.cat([extra_data, data], dim=0)\n",
    "\n",
    "  split_idx = int(len(data) * training_ratio)\n",
    "\n",
    "  data_train = data[:split_idx]\n",
    "  data_test = data[split_idx:]\n",
    "\n",
    "  if force_adjacent:\n",
    "    data_train = shuffle_data(data_train)\n",
    "    data_test = shuffle_data(data_test)\n",
    "\n",
    "  return data_train, data_test\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "def make_generator_from_data(data: List[Any], batch_size: int = 128) -> Iterable[List[Any]]:\n",
    "  \"\"\"\n",
    "  Returns a generator that yields slices of length `batch_size` from a list.\n",
    "\n",
    "  Args:\n",
    "      data: The input list to be split into batches.\n",
    "      batch_size: The size of each batch.\n",
    "\n",
    "  Yields:\n",
    "      A slice of the input list of length `batch_size`. The final slice may be shorter if the\n",
    "      length of the list is not evenly divisible by `batch_size`.\n",
    "  \"\"\"\n",
    "  data = shuffle_data(data)\n",
    "  for i in range(0,len(data), batch_size):\n",
    "    yield data[i:i+batch_size]\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "def make_wandb_config(\n",
    "    model:HookedTransformer,\n",
    "    optimizer_kwargs: dict,\n",
    "    n_epochs=100,\n",
    "    batch_size=128,\n",
    "    batches_per_epoch=10,\n",
    "    adjacent_fraction=0,\n",
    "    use_complete_data=True,\n",
    "    device=None,\n",
    "    **kwargs):\n",
    "  return {\n",
    "      'model.cfg':model.cfg.to_dict(),\n",
    "      'optimizer.cfg':optimizer_kwargs,\n",
    "      'n_epochs':n_epochs,\n",
    "      'batch_size':batch_size,\n",
    "      'batches_per_epoch':batches_per_epoch,\n",
    "      'adjacent_fraction':adjacent_fraction,\n",
    "      'use_complete_data':use_complete_data,\n",
    "      'device':device,\n",
    "    }\n",
    "\n",
    "def load_model(model: HookedTransformer, model_pth_path: str):\n",
    "  try:\n",
    "    cached_data = torch.load(model_pth_path)\n",
    "    model.load_state_dict(cached_data['model'])\n",
    "    #model_checkpoints = cached_data[\"checkpoints\"]\n",
    "    #checkpoint_epochs = cached_data[\"checkpoint_epochs\"]\n",
    "    #test_losses = cached_data['test_losses']\n",
    "    train_losses = cached_data['train_losses']\n",
    "    #train_indices = cached_data[\"train_indices\"]\n",
    "    #test_indices = cached_data[\"test_indices\"]\n",
    "    return train_losses, model_pth_path\n",
    "  except Exception as e:\n",
    "    print(f'Could not load model from {model_pth_path}:\\n', e)\n",
    "\n",
    "def train_or_load_model(\n",
    "      model_name:str,\n",
    "      model:HookedTransformer,\n",
    "      loss_fn,\n",
    "      acc_fn,\n",
    "      train_data_gen_maybe_lambda,\n",
    "      data_test,\n",
    "      n_epochs=100,\n",
    "      batches_per_epoch=10,\n",
    "      device=None,\n",
    "      wandb_project=None,\n",
    "      save_model=True,\n",
    "      model_pth_path=None,\n",
    "      deterministic: bool = False,\n",
    "      optimizer=torch.optim.Adam,\n",
    "      optimizer_kwargs={'lr':1e-3, 'betas': (.9, .999)},\n",
    "      train_data_gen_is_lambda: bool = False,\n",
    "      loss_fn_kwargs={'return_per_token':True},\n",
    "      print_every: Optional[int] = 10,\n",
    "      log_acc: bool = False,\n",
    "      force_train: bool = False,\n",
    "      overwrite_data: bool = False,\n",
    "      model_description: str = \"trained model\",\n",
    "      wandb_entity:str = DEFAULT_WANDB_ENTITY,\n",
    "      fail_if_cant_load: bool = False,\n",
    "      save_in_google_drive: bool = False,\n",
    "      **kwargs, # kwargs for **locals() below\n",
    "  ):\n",
    "  if force_train and fail_if_cant_load: raise ValueError(f\"force_train is {force_train} and fail_if_cant_load is {fail_if_cant_load}\")\n",
    "  if device is None: device = default_device(deterministic=deterministic)\n",
    "\n",
    "  pth_base_path = get_pth_base_path(save_in_google_drive=save_in_google_drive, create=True)\n",
    "  if model_pth_path is None:\n",
    "    datetime_str = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    model_pth_path = pth_base_path / f'{model_name}-{model.cfg.n_ctx}-epochs-{n_epochs}-{datetime_str}.pth'\n",
    "\n",
    "  if not force_train and os.path.exists(model_pth_path):\n",
    "    res = load_model(model, model_pth_path)\n",
    "    if res is not None: return res\n",
    "\n",
    "  if wandb_project is not None:\n",
    "    wandb_model_path = f\"{wandb_entity}/{wandb_project}/{model_name}:latest\"\n",
    "    if not force_train:\n",
    "      model_dir = None\n",
    "      try:\n",
    "        api = wandb.Api()\n",
    "        model_at = api.artifact(wandb_model_path)\n",
    "        model_dir = Path(model_at.download())\n",
    "      except Exception as e:\n",
    "        print(f'Could not load model {wandb_model_path} from wandb:\\n', e)\n",
    "      if model_dir is not None:\n",
    "        for model_path in model_dir.glob('*.pth'):\n",
    "          res = load_model(model, model_path)\n",
    "          if res is not None: return res\n",
    "\n",
    "  assert not fail_if_cant_load, f\"Couldn't load model from {model_pth_path}{f' or wandb ({wandb_model_path})' if wandb_project is not None else ''}, and fail_if_cant_load is {fail_if_cant_load}\"\n",
    "\n",
    "  if wandb_project is not None:\n",
    "    config_info = make_wandb_config(**locals())\n",
    "    run = wandb.init(project=wandb_project, entity=wandb_entity, config=config_info, job_type=\"train\")\n",
    "\n",
    "  optimizer = optimizer(model.parameters(), **optimizer_kwargs)\n",
    "  train_data_gen_lambda = (lambda: train_data_gen_maybe_lambda) if not train_data_gen_is_lambda else train_data_gen_maybe_lambda\n",
    "\n",
    "  train_losses = []\n",
    "\n",
    "  pbar = tqdm.tqdm(range(n_epochs))\n",
    "  for epoch in pbar:\n",
    "    train_data_gen = train_data_gen_lambda()\n",
    "    epoch_losses = []\n",
    "    for _ in range(batches_per_epoch):\n",
    "      tokens = next(train_data_gen)\n",
    "      logits = model(tokens)\n",
    "      losses = loss_fn(logits, tokens, **loss_fn_kwargs)\n",
    "      losses.mean().backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "      epoch_losses.extend(losses.detach().cpu().numpy())\n",
    "\n",
    "    train_losses.append(np.mean(epoch_losses))\n",
    "\n",
    "    if print_every and epoch % print_every == 0:\n",
    "      pbar.set_description(f'Epoch {epoch} train loss: {train_losses[-1]:.5e}')\n",
    "\n",
    "    if wandb_project is not None:\n",
    "      log_data = {'train_loss': train_losses[-1]}\n",
    "      if log_acc: log_data['train_acc'] = acc_fn(model(tokens), tokens)\n",
    "      wandb.log(log_data)\n",
    "\n",
    "  model.eval()\n",
    "  logits = model(data_test)\n",
    "  acc = acc_fn(logits, data_test)\n",
    "\n",
    "  print(f\"Test accuracy after training: {acc}\")\n",
    "\n",
    "  if save_model:\n",
    "    data = {\n",
    "       \"model\":model.state_dict(),\n",
    "       \"config\": model.cfg,\n",
    "       \"train_losses\": train_losses,\n",
    "       }\n",
    "    if overwrite_data or not os.path.exists(model_pth_path):\n",
    "      torch.save(data, model_pth_path)\n",
    "      if wandb_project is not None:\n",
    "        trained_model_artifact = wandb.Artifact(\n",
    "            model_name, type=\"model\", description=model_description, metadata=model.cfg.to_dict())\n",
    "        trained_model_artifact.add_file(model_pth_path)\n",
    "        run.log_artifact(trained_model_artifact)\n",
    "    elif wandb_project is not None:\n",
    "      print(f\"Warning: {model_pth_path} already exists, saving model directly\")\n",
    "      run.log_artifact(data)\n",
    "\n",
    "  if wandb_project is not None:\n",
    "    run.finish()\n",
    "\n",
    "  return train_losses, model_pth_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title max_of_n\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import tqdm.auto as tqdm\n",
    "import wandb\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def loss_fn(\n",
    "    logits, # [batch, pos, d_vocab]\n",
    "    tokens, # [batch, pos]\n",
    "    return_per_token=False,\n",
    "    device=DEVICE,\n",
    "  ):\n",
    "  logits = logits[:, -1, :].to(device)\n",
    "  true_maximum = torch.max(tokens.to(device), dim=1)[0]\n",
    "  log_probs = logits.log_softmax(-1)\n",
    "  correct_log_probs = log_probs.gather(-1, true_maximum.unsqueeze(-1))\n",
    "  if return_per_token:\n",
    "    return -correct_log_probs.squeeze()\n",
    "  return -correct_log_probs.mean()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def acc_fn(\n",
    "    logits, # [batch, pos, d_vocab]\n",
    "    tokens, # [batch, pos]\n",
    "    return_per_token=False,\n",
    "    device=DEVICE,\n",
    "  ):\n",
    "  pred_logits = logits[:, -1, :].to(device)\n",
    "  pred_tokens = torch.argmax(pred_logits, dim=1)\n",
    "  true_maximum = torch.max(tokens.to(device), dim=1)[0]\n",
    "  if return_per_token:\n",
    "    return (pred_tokens == true_maximum).float()\n",
    "  return (pred_tokens == true_maximum).float().mean().item()\n",
    "\n",
    "\n",
    "def large_data_gen(n_digits, sequence_length=6, batch_size=128, context=\"train\", device=DEVICE, adjacent_fraction=0):\n",
    "  if context == \"train\":\n",
    "    seed = 5\n",
    "  else:\n",
    "    seed = 6\n",
    "  torch.manual_seed(seed)\n",
    "  while True:\n",
    "    result = torch.randint(0, n_digits, (batch_size, sequence_length)).to(device)\n",
    "    if adjacent_fraction == 0: yield result\n",
    "    else:\n",
    "      adjacent = torch.randint(0, n_digits, (batch_size,))\n",
    "      adjacent = adjacent.unsqueeze(1).repeat(1, sequence_length)\n",
    "      # in half the rows, replace a random column with n+1\n",
    "      rows_to_change = torch.randperm(batch_size)[:batch_size // 2]\n",
    "      cols_to_change = torch.randint(0, sequence_length, (batch_size // 2,))\n",
    "      adjacent[rows_to_change, cols_to_change] += 1\n",
    "      adjacent %= n_digits\n",
    "      adjacent = adjacent.to(device)\n",
    "      mask = torch.rand(batch_size) < adjacent_fraction\n",
    "      result[mask] = adjacent[mask]\n",
    "      yield result\n",
    "\n",
    "def make_wandb_config(\n",
    "    model:HookedTransformer,\n",
    "    n_epochs=100,\n",
    "    batch_size=128,\n",
    "    batches_per_epoch=10,\n",
    "    adjacent_fraction=0,\n",
    "    use_complete_data=True,\n",
    "    device=DEVICE,\n",
    "    lr=1e-3,\n",
    "    betas=(.9, .999),\n",
    "    **kwargs):\n",
    "  return {\n",
    "      'model.cfg':model.cfg.to_dict(),\n",
    "      'optimizer.cfg':{\n",
    "        'lr':lr,\n",
    "        'betas':betas,\n",
    "      },\n",
    "      'n_epochs':n_epochs,\n",
    "      'batch_size':batch_size,\n",
    "      'batches_per_epoch':batches_per_epoch,\n",
    "      'adjacent_fraction':adjacent_fraction,\n",
    "      'use_complete_data':use_complete_data,\n",
    "      'device':device,\n",
    "    }\n",
    "\n",
    "def train_model(\n",
    "    model:HookedTransformer,\n",
    "    n_epochs=100,\n",
    "    batch_size=128,\n",
    "    batches_per_epoch=10,\n",
    "    adjacent_fraction=0,\n",
    "    use_complete_data=True,\n",
    "    device=DEVICE,\n",
    "    use_wandb=False,\n",
    "    wandb_project=None,\n",
    "    save_model=True,\n",
    "  ):\n",
    "  lr = 1e-3\n",
    "  betas = (.9, .999)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)\n",
    "  n_digits, sequence_length = model.cfg.d_vocab, model.cfg.n_ctx\n",
    "  train_losses = []\n",
    "  if wandb_project is not None:\n",
    "    config_info = make_wandb_config(model, **locals())\n",
    "    run = wandb.init(project=wandb_project, config=config_info, job_type=\"train\")\n",
    "\n",
    "  if use_complete_data:\n",
    "    data_train, data_test = make_testset_trainset(model, force_adjacent=adjacent_fraction > 0)\n",
    "    train_data_gen_gen = lambda: make_generator_from_data(data_train, batch_size=batch_size)\n",
    "  else:\n",
    "    train_data_gen = large_data_gen(n_digits=n_digits, sequence_length=sequence_length, batch_size=batch_size, context=\"train\", device=device, adjacent_fraction=adjacent_fraction)\n",
    "    test_data_gen = large_data_gen(n_digits=n_digits, sequence_length=sequence_length, batch_size=batch_size * 20, context=\"test\", adjacent_fraction=adjacent_fraction)\n",
    "    data_test = next(test_data_gen)\n",
    "\n",
    "  for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    if use_complete_data:\n",
    "      train_data_gen = train_data_gen_gen()\n",
    "    epoch_losses = []\n",
    "    for _ in range(batches_per_epoch):\n",
    "      tokens = next(train_data_gen)\n",
    "      logits = model(tokens)\n",
    "      losses = loss_fn(logits, tokens, return_per_token=True)\n",
    "      losses.mean().backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "      epoch_losses.extend(losses.detach().cpu().numpy())\n",
    "\n",
    "    train_losses.append(np.mean(epoch_losses))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "      print(f'Epoch {epoch} train loss: {train_losses[-1]}')\n",
    "\n",
    "    if use_wandb or wandb_project is not None:\n",
    "      wandb.log({'train_loss': train_losses[-1]})\n",
    "\n",
    "  model.eval()\n",
    "  logits = model(data_test)\n",
    "  acc = acc_fn(logits, data_test)\n",
    "\n",
    "  print(f\"Test accuracy after training: {acc}\")\n",
    "\n",
    "  if save_model and (use_wandb or wandb_project is not None):\n",
    "    wandb.log_artifact(model)\n",
    "\n",
    "  if wandb_project is not None:\n",
    "    run.finish()\n",
    "\n",
    "  return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title train_max_of_2\n",
    "import sys\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "DETERMINISTIC = True # @param\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() and not DETERMINISTIC else \"cpu\"\n",
    "N_LAYERS = 1 # @param\n",
    "N_HEADS = 1 # @param\n",
    "D_MODEL = 32 # @param\n",
    "D_HEAD = 32 # @param\n",
    "D_MLP = None # @param\n",
    "D_VOCAB = 64 # @param\n",
    "SEED = 123 # @param\n",
    "N_EPOCHS = 1500 # @param\n",
    "N_CTX = 2 # @param\n",
    "FORCE_ADJACENT = True # @param\n",
    "BATCH_SIZE = 128 # @param\n",
    "FAIL_IF_CANT_LOAD = '--fail-if-cant-load' in sys.argv[1:] # @param\n",
    "\n",
    "ALWAYS_TRAIN_MODEL = False # @param\n",
    "SAVE_IN_GOOGLE_DRIVE = False # @param\n",
    "OVERWRITE_DATA = False # @param\n",
    "TRAIN_MODEL_IF_CANT_LOAD = True # @param\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "simpler_cfg = HookedTransformerConfig(\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    d_head=D_HEAD,\n",
    "    n_ctx=N_CTX,\n",
    "    d_vocab=D_VOCAB,\n",
    "    seed=SEED,\n",
    "    device=DEVICE,\n",
    "    attn_only=True,\n",
    "    normalization_type=None,\n",
    ")\n",
    "# %%\n",
    "\n",
    "model = HookedTransformer(simpler_cfg).to(DEVICE)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"b_\" in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "model_is_trained = False\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "def train(fail_if_cant_load=FAIL_IF_CANT_LOAD, train_if_cant_load=TRAIN_MODEL_IF_CANT_LOAD, overwrite_data=OVERWRITE_DATA,\n",
    "          always_train_model=ALWAYS_TRAIN_MODEL,\n",
    "          wandb_entity=DEFAULT_WANDB_ENTITY,\n",
    "          save_in_google_drive=SAVE_IN_GOOGLE_DRIVE):\n",
    "\n",
    "    global model_is_trained\n",
    "\n",
    "    data_train, data_test = make_testset_trainset(model, force_adjacent=FORCE_ADJACENT)\n",
    "    train_data_gen_gen = lambda: make_generator_from_data(data_train, batch_size=BATCH_SIZE)\n",
    "\n",
    "    training_losses, model_pth_path = train_or_load_model(\n",
    "        f'neural-net-coq-interp-max-{model.cfg.n_ctx}-epochs-{N_EPOCHS}',\n",
    "        model,\n",
    "        loss_fn=loss_fn,\n",
    "        acc_fn=acc_fn,\n",
    "        train_data_gen_maybe_lambda=train_data_gen_gen,\n",
    "        train_data_gen_is_lambda=True,\n",
    "        data_test=data_test,\n",
    "        n_epochs=N_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        adjacent_fraction=1,\n",
    "        use_complete_data=True,\n",
    "        batches_per_epoch=10,\n",
    "        wandb_project=f'neural-net-coq-interp-max-{model.cfg.n_ctx}-epochs-{N_EPOCHS}',\n",
    "        deterministic=DETERMINISTIC,\n",
    "        save_in_google_drive=save_in_google_drive,\n",
    "        overwrite_data=overwrite_data,\n",
    "        train_model_if_cant_load=train_if_cant_load,\n",
    "        model_description=f\"trained max of {model.cfg.n_ctx} model\",\n",
    "        save_model=True,\n",
    "        force_train=always_train_model,\n",
    "        wandb_entity=wandb_entity,\n",
    "        fail_if_cant_load=fail_if_cant_load,\n",
    "    )\n",
    "\n",
    "    model_is_trained = True\n",
    "    return training_losses, model_pth_path\n",
    "\n",
    "# %%\n",
    "\n",
    "def get_model(train_if_necessary = False,  **kwargs):\n",
    "\n",
    "    train(fail_if_cant_load = not train_if_necessary, train_if_cant_load = train_if_necessary, **kwargs)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title analysis_utils\n",
    "\n",
    "from typing import Callable, Iterable, List, Optional, Tuple\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformer_lens import HookedTransformer\n",
    "import transformer_lens.utils as utils\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.colors\n",
    "from plotly.subplots import make_subplots\n",
    "from inspect import signature\n",
    "import itertools\n",
    "\n",
    "# %%\n",
    "\n",
    "def linear_func(x, a, b):\n",
    "    \"\"\"Linear function: f(x) = a * x + b\"\"\"\n",
    "    return a * x + b\n",
    "linear_func.equation = lambda popt: f'y = {popt[0]:.3f}*x + {popt[1]:.3f}'\n",
    "\n",
    "def quadratic_func(x, a, b, c):\n",
    "    return a * x**2 + b * x + c\n",
    "quadratic_func.equation = lambda popt: f'y = {popt[0]:.3e}*x^2 + {popt[1]:.3f}*x + {popt[2]:.3f}'\n",
    "\n",
    "def cubic_func(x, a, b, c, d):\n",
    "    return a * x**3 + b * x**2 + c * x + d\n",
    "cubic_func.equation = lambda popt: f'y = {popt[0]:.3e}*x^3 + {popt[1]:.3e}*x^2 + {popt[2]:.3f}*x + {popt[3]:.3f}'\n",
    "\n",
    "def quartic_func(x, a, b, c, d, e):\n",
    "    return a * x**4 + b * x**3 + c * x**2 + d * x + e\n",
    "quartic_func.equation = lambda popt: f'y = {popt[0]:.3e}*x^4 + {popt[1]:.3e}*x^3 + {popt[2]:.3e}*x^2 + {popt[3]:.3f}*x + {popt[4]:.3f}'\n",
    "\n",
    "def quintic_func(x, a, b, c, d, e, f):\n",
    "    return a * x**5 + b * x**4 + c * x**3 + d * x**2 + e * x + f\n",
    "quintic_func.equation = lambda popt: f'y = {popt[0]:.3e}*x^5 + {popt[1]:.3e}*x^4 + {popt[2]:.3e}*x^3 + {popt[3]:.3e}*x^2 + {popt[4]:.3f}*x + {popt[5]:.3f}'\n",
    "\n",
    "def absolute_shift_func(x, a, b, c):\n",
    "    return a * np.abs(x - b) + c\n",
    "absolute_shift_func.equation = lambda popt: f'y = {popt[0]:.3f}*|x - {popt[1]:.3f}| + {popt[2]:.3f}'\n",
    "\n",
    "def linear_sinusoid_func(x, a, b, c, d):\n",
    "    return (a * x + b) * np.sin(c * x + d)\n",
    "linear_sinusoid_func.equation = lambda popt: f'y = ({popt[0]:.3f}*x + {popt[1]:.3f}) * sin({popt[2]:.3f}*x + {popt[3]:.3f})'\n",
    "\n",
    "def quadratic_sinusoid_func(x, a, b, c, d, e):\n",
    "    return (a * x**2 + b * x + c) * np.sin(d * x + e)\n",
    "quadratic_sinusoid_func.equation = lambda popt: f'y = ({popt[0]:.3f}*x^2 + {popt[1]:.3f}*x + {popt[2]:.3f}) * sin({popt[3]:.3f}*x + {popt[4]:.3f})'\n",
    "\n",
    "def absolute_shift_sinusoid_func(x, a, b, c, d, e):\n",
    "    return (a * np.abs(x - b) + c) * np.sin(d * x + e)\n",
    "absolute_shift_sinusoid_func.equation = lambda popt: f'y = ({popt[0]:.3f}*|x - {popt[1]:.3f}| + {popt[2]:.3f}) * sin({popt[3]:.3f}*x + {popt[4]:.3f})'\n",
    "\n",
    "def linear_abs_sinusoid_func(x, a, b, c, d):\n",
    "    return (a * x + b) * np.abs(np.sin(c * x + d))\n",
    "linear_abs_sinusoid_func.equation = lambda popt: f'y = ({popt[0]:.3f}*x + {popt[1]:.3f}) * |sin({popt[2]:.3f}*x + {popt[3]:.3f})|'\n",
    "\n",
    "def quadratic_abs_sinusoid_func(x, a, b, c, d, e):\n",
    "    return (a * x**2 + b * x + c) * np.abs(np.sin(d * x + e))\n",
    "quadratic_abs_sinusoid_func.equation = lambda popt: f'y = ({popt[0]:.3f}*x^2 + {popt[1]:.3f}*x + {popt[2]:.3f}) * |sin({popt[3]:.3f}*x + {popt[4]:.3f})|'\n",
    "\n",
    "def absolute_shift_abs_sinusoid_func(x, a, b, c, d, e):\n",
    "    return (a * np.abs(x - b) + c) * np.abs(np.sin(d * x + e))\n",
    "absolute_shift_abs_sinusoid_func.equation = lambda popt: f'y = ({popt[0]:.3f}*|x - {popt[1]:.3f}| + {popt[2]:.3f}) * |sin({popt[3]:.3f}*x + {popt[4]:.3f})|'\n",
    "\n",
    "def sigmoid_func(x, K, B, M):\n",
    "    return K / (1 + np.exp(-B * (x - M)))\n",
    "sigmoid_func.equation = lambda popt: f'y = {popt[0]:.3f} / (1 + exp(-{popt[1]:.3f} * (x - {popt[2]:.3f})))'\n",
    "\n",
    "def inv_sigmoid_func(y, K, B, M):\n",
    "    return M - np.log(K / y - 1) / B\n",
    "inv_sigmoid_func.equation = lambda popt: f'x = {popt[2]:.3f} - ln({popt[0]:.3f} / y - 1) / {popt[1]:.3f}'\n",
    "\n",
    "def fit_name_of_func(fit_function):\n",
    "    fit_name = fit_function.__name__\n",
    "    if fit_name is not None and fit_name.endswith('_func'): fit_name = fit_name[:-len('_func')]\n",
    "    return fit_name\n",
    "\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", colorscale=\"RdBu\", **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=colorscale, labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", line_labels=None, showlegend=None, hovertemplate=None, **kwargs):\n",
    "    fig = px.line(utils.to_numpy(tensor), labels={\"index\":xaxis, \"value\":yaxis}, y=line_labels, **kwargs)\n",
    "    if showlegend is not None: fig.update_layout(showlegend=showlegend)\n",
    "    if hovertemplate is not None: fig.update_traces(hovertemplate=hovertemplate)\n",
    "    fig.show(renderer)\n",
    "\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)\n",
    "\n",
    "\n",
    "def hist(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.histogram(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "\n",
    "def pm_range(values):\n",
    "    return f\"{(values.max().item() + values.min().item()) / 2.0} ± {(values.max().item() - values.min().item()) / 2.0}\"\n",
    "\n",
    "\n",
    "def pm_mean_std(values):\n",
    "    return f\"{values.mean().item()} ± {values.std().item()}\"\n",
    "\n",
    "\n",
    "def summarize(values, name=None, histogram=False, renderer=None, hist_args={},\n",
    "              imshow_args=None, include_value=False, linear_fit=False,\n",
    "              fit_function=None, fit_equation=None, fit_name=None,\n",
    "              min=True, max=True, mean=True, median=True, range=True, range_size=True, firstn=None, abs_max=True):\n",
    "    if histogram:\n",
    "        hist_args_list = hist_args if isinstance(hist_args, list) else [hist_args]\n",
    "        for hist_args in hist_args_list:\n",
    "            hist_args = dict(hist_args)\n",
    "            if 'title' not in hist_args and name is not None: hist_args['title'] = f'Histogram of {name}'\n",
    "            if 'renderer' not in hist_args and renderer is not None: hist_args['renderer'] = renderer\n",
    "            if 'xaxis' not in hist_args: hist_args['xaxis'] = name if name is not None else 'Value'\n",
    "            if 'yaxis' not in hist_args: hist_args['yaxis'] = 'Count'\n",
    "            hist(values, **hist_args)\n",
    "\n",
    "    if imshow_args is not None:\n",
    "        imshow_args = dict(imshow_args)\n",
    "        if 'title' not in imshow_args and name is not None: imshow_args['title'] = name\n",
    "        if 'renderer' not in imshow_args and renderer is not None: imshow_args['renderer'] = renderer\n",
    "        if 'xaxis' not in imshow_args and name is not None: imshow_args['xaxis'] = f'({name}).shape[1]'\n",
    "        if 'yaxis' not in imshow_args and name is not None: imshow_args['yaxis'] = f'({name}).shape[0]'\n",
    "        if len(values.shape) == 1:\n",
    "            line(values, **imshow_args)\n",
    "        else:\n",
    "            imshow(values, **imshow_args)\n",
    "\n",
    "    if fit_function is None and linear_fit: fit_function = linear_func\n",
    "    if fit_equation is None and fit_function is not None: fit_equation = fit_function.equation\n",
    "    if fit_function is not None:\n",
    "        assert len(values.shape) in (1, 2)\n",
    "        if len(values.shape) == 1:\n",
    "            x_vals = np.arange(values.shape[0])\n",
    "            y_vals = utils.to_numpy(values)\n",
    "            aggregated = ''\n",
    "        else:\n",
    "            x_vals = np.tile(np.arange(values.shape[1]), values.shape[0])\n",
    "            y_vals = utils.to_numpy(values.flatten())\n",
    "            aggregated = 'Aggregated '\n",
    "        name_space = '' if name is None else f'{name} '\n",
    "        if fit_name is None:\n",
    "            fit_name = fit_function.__name__\n",
    "            if fit_name is not None and fit_name.endswith('_func'): fit_name = fit_name[:-len('_func')]\n",
    "        fit_name_space = '' if not fit_name else f'{fit_name} '\n",
    "        fit_title = f\"{aggregated}{name_space}Data and {fit_name_space}Fit\"\n",
    "        resid_title = f\"{aggregated}{name_space}Residual Errors\"\n",
    "\n",
    "        # Fit linear regression to the aggregated data\n",
    "        popt, _ = curve_fit(fit_function, x_vals, y_vals)\n",
    "\n",
    "        # Create a subplot with 1 row and 2 columns\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 6))  # Adjust the figure size to your liking\n",
    "\n",
    "        # Scatter plot the data & best fit line on the first subplot\n",
    "        axs[0].scatter(x_vals, y_vals, label='Data', alpha=0.5, s=1)\n",
    "        axs[0].plot(x_vals, fit_function(x_vals, *popt), 'r-', label=f'Fit: {fit_equation(popt)}')\n",
    "        axs[0].set_title(fit_title)\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Plot residual errors on the second subplot\n",
    "        residuals = y_vals - fit_function(x_vals, *popt)\n",
    "        order_indices = np.argsort(x_vals)\n",
    "        axs[1].scatter(x_vals[order_indices], residuals[order_indices], c='b', alpha=0.5)\n",
    "        axs[1].set_title(resid_title)\n",
    "\n",
    "        # Adjust the layout\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    res = {}\n",
    "    if include_value: res['value'] = values.detach().clone().cpu()\n",
    "    if min: res['min'] = values.min().item()\n",
    "    if max: res['max'] = values.max().item()\n",
    "    if mean: res['mean'] = pm_mean_std(values.float())\n",
    "    if median: res['median'] = values.median().item()\n",
    "    if range: res['range'] = pm_range(values)\n",
    "    if range_size: res['range_size'] = values.max().item() - values.min().item()\n",
    "    if firstn is not None: res[f'first {firstn}'] = values[:firstn]\n",
    "    if abs_max: res['abs(max)'] = values.abs().max().item()\n",
    "    if fit_function is not None: res['fit_equation'] = f'y = {popt[0]}*x + {popt[1]}'\n",
    "    if fit_function is not None: res['range_residuals'] = pm_range(residuals)\n",
    "    if fit_function is not None: res['residuals'] = residuals[order_indices]\n",
    "    if fit_function is not None: res['fit_params'] = popt\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "#list(zip(all_integers[~correct_idxs], all_integers_ans[~correct_idxs]))\n",
    "\n",
    "def center_by_mid_range(tensor: torch.Tensor, dim: Optional[int] = None) -> torch.Tensor:\n",
    "    maxv, minv = tensor.max(dim=dim, keepdim=True).values, tensor.min(dim=dim, keepdim=True).values\n",
    "    return tensor - (maxv + minv) / 2.0\n",
    "\n",
    "# # Simpler Model Interpretabiltiy\n",
    "\n",
    "# ## Calculating how much slack we have\n",
    "\n",
    "# Let's find out what the actual logits are, and how much slack we have on errors\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def analyze_svd(M, descr='', scale_by_singular_value=True, colorscale='Picnic_r', singular_color='blue', renderer=None):\n",
    "    U, S, Vh = torch.linalg.svd(M)\n",
    "    V = Vh.T\n",
    "    if scale_by_singular_value:\n",
    "        U = U * S[None, :].sqrt()\n",
    "        V = V * S[:, None].sqrt()\n",
    "    if descr: descr = f' for {descr}'\n",
    "\n",
    "    fig = make_subplots(rows=1, cols=3, subplot_titles=[\"U\", \"Singular Values\", \"V\"])\n",
    "    uzmax, vzmax = U.abs().max().item(), V.abs().max().item()\n",
    "    fig.add_trace(go.Heatmap(z=utils.to_numpy(U), zmin=-uzmax, zmax=uzmax, colorscale=colorscale,\n",
    "                             showscale=False,\n",
    "                            hovertemplate=\"U: %{y}<br>Singular Index: %{x}<br>Value: %{z}<extra></extra>\"\n",
    "                            ),\n",
    "                row=1, col=1)\n",
    "    fig.add_trace(go.Heatmap(z=utils.to_numpy(V), colorscale=colorscale, zmin=-vzmax, zmax=vzmax,\n",
    "                             showscale=False,\n",
    "                            hovertemplate=\"V: %{x}<br>Singular Index: %{y}<br>Value: %{z}<extra></extra>\",\n",
    "                            ),\n",
    "                row=1, col=3)\n",
    "    fig.add_trace(go.Scatter(x=np.arange(S.shape[0]), y=utils.to_numpy(S),\n",
    "                            mode='lines+markers',\n",
    "                            marker=dict(color=singular_color),\n",
    "                            line=dict(color=singular_color),\n",
    "                            hovertemplate=\"Singular Value: %{y}<br>Singular Index: %{x}<extra></extra>\",\n",
    "                            ), row=1, col=2)\n",
    "    fig.update_layout(title=f\"SVD{descr}\") #, margin=dict(l=150, r=150))\n",
    "\n",
    "\n",
    "    fig.update_yaxes(range=[0, None], row=1, col=2)\n",
    "    # fig.update_yaxes(range=[0, None], row=1, col=2)\n",
    "    # fig.update_layout(yaxis_scaleanchor=\"x\")\n",
    "    fig.update_yaxes(scaleanchor='x', autorange='reversed', row=1, col=1)\n",
    "    fig.update_yaxes(scaleanchor='x', autorange='reversed', row=1, col=3)\n",
    "\n",
    "    # fig.update_xaxes(scaleanchor='y', scaleratio=1, range=[0, U.shape[0]], row=1, col=1)\n",
    "    # fig.update_yaxes(scaleanchor='x', scaleratio=1, range=[0, U.shape[1]], row=1, col=1)\n",
    "\n",
    "    # fig.update_xaxes(scaleanchor='y', scaleratio=1, range=[0, None], row=1, col=2)\n",
    "    # fig.update_yaxes(scaleanchor='x', scaleratio=1, range=[0, S.shape[0]], row=1, col=2)\n",
    "\n",
    "    # fig.update_xaxes(scaleanchor='y', scaleratio=1, range=[0, Vh.shape[0]], row=1, col=3)\n",
    "    # fig.update_yaxes(scaleanchor='x', scaleratio=1, range=[0, Vh.shape[1]], row=1, col=3)\n",
    "\n",
    "    # fig.update_xaxes(range=[0, None], row=1, col=1)\n",
    "    # fig.update_xaxes(range=[0, None], row=1, col=2)\n",
    "    # fig.update_xaxes(range=[0, None], row=1, col=3)\n",
    "\n",
    "    # fig.update_yaxes(range=[0, None], row=1, col=1)\n",
    "    # fig.update_yaxes(range=[0, None], row=1, col=2)\n",
    "    # fig.update_yaxes(range=[0, None], row=1, col=3)\n",
    "\n",
    "    # fig.update_yaxes(title_text=\"Query Token\", row=1, col=1)\n",
    "    fig.update_yaxes(range=[0, None], row=1, col=2)\n",
    "    # fig.update_yaxes(title_text=\"Key Token\", row=1, col=3)\n",
    "\n",
    "    fig.show(renderer)\n",
    "\n",
    "\n",
    "    # line(S, title=f\"Singular Values{descr}\")\n",
    "    # imshow(U, title=f\"Principal Components on U{descr}\")\n",
    "    # imshow(Vh, title=f\"Principal Components on Vh{descr}\")\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "@torch.no_grad()\n",
    "def make_fit(values: torch.Tensor, fit_function, exclude_count=None):\n",
    "    assert len(values.shape) in (1, 2)\n",
    "    if len(values.shape) == 1:\n",
    "        x_vals = np.arange(values.shape[0])\n",
    "        y_vals = utils.to_numpy(values)\n",
    "    else:\n",
    "        x_vals = np.tile(np.arange(values.shape[1]), values.shape[0])\n",
    "        y_vals = utils.to_numpy(values.flatten())\n",
    "\n",
    "    x_vals_fit, y_vals_fit = x_vals, y_vals\n",
    "    if exclude_count is not None: x_vals_fit, y_vals_fit = x_vals[exclude_count:-exclude_count], y_vals[exclude_count:-exclude_count]\n",
    "    popt, _ = curve_fit(fit_function, x_vals_fit, y_vals_fit)\n",
    "\n",
    "    residuals = y_vals - fit_function(x_vals, *popt)\n",
    "    order_indices = np.argsort(x_vals)\n",
    "\n",
    "    return popt, (x_vals, y_vals), (x_vals, fit_function(x_vals, *popt)), (x_vals[order_indices], residuals[order_indices])\n",
    "\n",
    "\n",
    "def make_fit_traces(values: torch.Tensor, fit_function, exclude_count=None, fit_equation: Optional[Callable] = None, reference_lines: Optional[List[Tuple[str, float]]] = None, reference_colors=plotly.colors.qualitative.Dark24):\n",
    "    popt, points, fit, resid = make_fit(values, fit_function, exclude_count=exclude_count)\n",
    "    if fit_equation is None: fit_equation = fit_function.equation\n",
    "    if reference_lines is None: reference_lines = []\n",
    "    reference_line_traces = \\\n",
    "        [go.Scatter(x=np.arange(points[0].shape[0]), y=np.full(points[0].shape, val), name=name, mode='lines', line=dict(color=color, dash='dash'),\n",
    "                hovertemplate=f'{val}<extra>{name}</extra>',\n",
    "                showlegend=False, legendgroup=fit_function.__name__)\n",
    "        for (name, val), color in zip(reference_lines, itertools.cycle(reference_colors))]\n",
    "    # , size=1\n",
    "    return popt, \\\n",
    "            [go.Scatter(x=points[0], y=points[1], name='Data', mode='markers', marker=dict(color='red', opacity=0.5), showlegend=True, legendgroup=fit_function.__name__),\n",
    "            go.Scatter(x=fit[0], y=fit[1], name=f'Fit: {fit_equation(popt)}', mode='lines', line=dict(color='blue'), showlegend=True, legendgroup=fit_function.__name__),\n",
    "            go.Scatter(x=resid[0], y=resid[1], name='Residuals', mode='markers', marker=dict(color='red', opacity=0.5), showlegend=False)], \\\n",
    "            reference_line_traces\n",
    "\n",
    "def show_fits(values: torch.Tensor, name: str, fit_funcs: Iterable[Callable], do_exclusions=True, renderer=None, **kwargs):\n",
    "    assert len(values.shape) == 1\n",
    "    fit_funcs = list(fit_funcs)\n",
    "    fig = make_subplots(rows=len(fit_funcs), cols=2,\n",
    "                        subplot_titles=[title\n",
    "                                        for fit_func in fit_funcs\n",
    "                                        for title in (f\"{fit_name_of_func(fit_func)} Fit\", f\"Residuals\")])\n",
    "    for i, fit_func in enumerate(fit_funcs):\n",
    "        popt, (points, fit, resid), reference_line_traces = make_fit_traces(values, fit_func, exclude_count=None, **kwargs)\n",
    "        fig.add_trace(points, row=i+1, col=1)\n",
    "        fig.add_trace(fit, row=i+1, col=1)\n",
    "        fig.add_trace(resid, row=i+1, col=2)\n",
    "        for trace in reference_line_traces:\n",
    "            fig.add_trace(trace, row=i+1, col=1)\n",
    "    fig.update_layout(\n",
    "        title=f\"{name} Data & Fit\",\n",
    "        legend=dict(\n",
    "            bgcolor='rgba(255,255,255,0.5)',\n",
    "            yanchor=\"middle\",\n",
    "            y=0.5,  # Y=1 anchors the legend to the top of the plot area\n",
    "            xanchor=\"left\",\n",
    "            x=0\n",
    "        ),\n",
    "        height=300 * len(fit_funcs) + 100,\n",
    "    )\n",
    "\n",
    "    if do_exclusions:\n",
    "        max_param_count = max([len(signature(fit_func).parameters) for fit_func in fit_funcs])\n",
    "        frames = [go.Frame(data=[trace\n",
    "                                for fit_func in fit_funcs\n",
    "                                for trace_list in make_fit_traces(values, fit_func, exclude_count=exclude_count, **kwargs)[1:]\n",
    "                                for trace in trace_list],\n",
    "                            name=(str(exclude_count) if exclude_count is not None else \"0\"),\n",
    "        ) for exclude_count in [None] + list(range(1, (values.shape[0] - max_param_count) // 2))]\n",
    "\n",
    "        fig.frames = frames\n",
    "\n",
    "        sliders = [dict(\n",
    "            active=0,\n",
    "            yanchor='top',\n",
    "            xanchor='left',\n",
    "            currentvalue=dict(font=dict(size=20), prefix='# End Points to Exclude:', visible=True, xanchor='right'),\n",
    "            transition=dict(duration=0),\n",
    "            pad=dict(b=10, t=50),\n",
    "            len=0.9,\n",
    "            x=0.1,\n",
    "            y=0,\n",
    "            steps=[dict(args=[[frame.name], dict(mode='immediate', frame=dict(duration=0, redraw=True), transition=dict(duration=0))],\n",
    "                        method='animate',\n",
    "                        label=frame.name) for frame in fig.frames]\n",
    "        )]\n",
    "\n",
    "        fig.update_layout(sliders=sliders)\n",
    "\n",
    "    fig.show(renderer)\n",
    "\n",
    "# %%\n",
    "\n",
    "# ## Negligibility of W_E @ W_U\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def calculate_embed_overlap(model: HookedTransformer, renderer=None):\n",
    "    W_U, W_E = model.W_U, model.W_E\n",
    "    d_model, d_vocab = model.cfg.d_model, model.cfg.d_vocab\n",
    "    assert W_U.shape == (d_model, d_vocab)\n",
    "    assert W_E.shape == (d_vocab, d_model)\n",
    "    res = (W_E @ W_U).detach()\n",
    "    self_overlap = res.diag()\n",
    "    imshow(res, renderer=renderer)\n",
    "    line(self_overlap, renderer=renderer)\n",
    "    statistics = [\n",
    "        ('overlap', res),\n",
    "        ('self-overlap', self_overlap),\n",
    "        ('self-overlap after 0', self_overlap[1:]),\n",
    "    ]\n",
    "    return {name: summarize(value, name=name, include_value=True) for name, value in statistics}\n",
    "\n",
    "\n",
    "# ## Negligibility of W_pos @ W_U\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def calculate_pos_embed_overlap(model: HookedTransformer, renderer=None):\n",
    "    W_U, W_pos = model.W_U, model.W_pos\n",
    "    d_model, d_vocab, n_ctx = model.cfg.d_model, model.cfg.d_vocab, model.cfg.n_ctx\n",
    "    assert W_U.shape == (d_model, d_vocab)\n",
    "    assert W_pos.shape == (n_ctx, d_model)\n",
    "    res = (W_pos @ W_U).detach()\n",
    "    imshow(res, renderer=renderer)\n",
    "\n",
    "    statistics = [\n",
    "        ('pos_embed_overlap', res),\n",
    "        ('pos_embed_overlap (pos -1)', res[-1,:]),\n",
    "    ]\n",
    "    return {name: summarize(value, name=name, include_value=True, linear_fit=True, renderer=renderer) for name, value in statistics}\n",
    "\n",
    "\n",
    "# ## Negligibility of (W_E + W_pos[-1]) @ W_U\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "def calculate_embed_and_pos_embed_overlap(model: HookedTransformer, renderer=None):\n",
    "    W_U, W_E, W_pos = model.W_U, model.W_E, model.W_pos\n",
    "    d_model, d_vocab, n_ctx = model.cfg.d_model, model.cfg.d_vocab, model.cfg.n_ctx\n",
    "    assert W_U.shape == (d_model, d_vocab)\n",
    "    assert W_pos.shape == (n_ctx, d_model)\n",
    "    assert W_E.shape == (d_vocab, d_model)\n",
    "    res = ((W_E + W_pos[-1,:]) @ W_U).detach()\n",
    "    self_overlap = res.diag()\n",
    "    centered_by_mid_range = center_by_mid_range(res, dim=-1)\n",
    "    centered = res - self_overlap[:,None]\n",
    "    centered_triu = centered.triu()\n",
    "    centered_tril = centered.tril()\n",
    "    centered_no_diag = centered.clone()\n",
    "    centered_no_diag.diagonal().fill_(-1000000)\n",
    "    centered_no_diag_after_0 = centered_no_diag[:,1:]\n",
    "    centered_no_diag = centered_no_diag[centered_no_diag != -1000000]\n",
    "    centered_no_diag_after_0 = centered_no_diag_after_0[centered_no_diag_after_0 != -1000000]\n",
    "    statistics = [\n",
    "        ('centered overlap (incl pos)', centered),\n",
    "        ('centered overlap (incl pos) triu', centered_triu),\n",
    "        ('centered overlap (incl pos) tril', centered_tril),\n",
    "        ('centered overlap after 0 (incl pos)', centered[:,1:]),\n",
    "        ('centered overlap after 0 (incl pos) no diag', centered_no_diag_after_0),\n",
    "        ('centered overlap only 0 (incl pos)', centered[:,0]),\n",
    "        ('centered overlap only 0 (incl pos) no diag', centered[1:,0]),\n",
    "        ('overlap (incl pos)', res),\n",
    "        ('self-overlap (incl pos)', self_overlap),\n",
    "        ('self-overlap after 0 (incl pos)', self_overlap[1:]),\n",
    "        ('centered overlap (incl pos) no diag', centered_no_diag),\n",
    "        ('centered by mid_range overlap (incl pos)', centered_by_mid_range),\n",
    "        ('centered by mid_range overlap after 0 (incl pos)', centered_by_mid_range[:,1:]),\n",
    "        ('centered by mid_range overlap only 0 (incl pos)', centered_by_mid_range[:,0]),\n",
    "        ('centered by mid_range overlap only 0 (incl pos) no diag', centered_by_mid_range[1:,0]),\n",
    "    ]\n",
    "    return {name: summarize(value, include_value=False, name=name, renderer=renderer, linear_fit=True,\n",
    "                            imshow_args={'yaxis':'input token', 'xaxis':'output token'},\n",
    "                            ) for name, value in statistics}\n",
    "\n",
    "\n",
    "def calculate_rowwise_embed_and_pos_embed_overlap(model: HookedTransformer, renderer=None):\n",
    "    \"\"\"\n",
    "    For `(W_E + W_pos[-1,:]) @ W_U`, we compute for each row the maximum absolute value of the following quantity:\n",
    "    - the largest negative value a number to the right of the diagonal is below the diagonal\n",
    "    - the largest positive value a number to the left  of the diagonal is above the diagonal\n",
    "    This is the exact value of the largest absolute error introduced in a given row.\n",
    "    \"\"\"\n",
    "    W_U, W_E, W_pos = model.W_U, model.W_E, model.W_pos\n",
    "    d_model, d_vocab, n_ctx = model.cfg.d_model, model.cfg.d_vocab, model.cfg.n_ctx\n",
    "    assert W_U.shape == (d_model, d_vocab)\n",
    "    assert W_pos.shape == (n_ctx, d_model)\n",
    "    assert W_E.shape == (d_vocab, d_model)\n",
    "    res = ((W_E + W_pos[-1,:]) @ W_U).detach()\n",
    "    self_overlap = res.diag()\n",
    "    centered = res - self_overlap[:,None]\n",
    "    centered_triu = centered.triu()\n",
    "    centered_tril = centered.tril()\n",
    "    diffs = centered_tril - centered_triu\n",
    "    imshow(res)\n",
    "    imshow(centered)\n",
    "    imshow(diffs)\n",
    "    # # max of positive differences to the right of the diagonal\n",
    "    # max_pos_diffs = torch.max(centered_triu, dim=-1).values\n",
    "    # # max of negative differences to the left of the diagonal\n",
    "    # max_neg_diffs = torch.min(centered_tril, dim=-1).values\n",
    "    # # stack the diffs\n",
    "    # diffs = torch.stack([max_neg_diffs, max_pos_diffs], dim=-1)\n",
    "    # summarize(diffs, name='rowwise diffs (positive and negative)', renderer=renderer)\n",
    "    # # take the max of the diffs\n",
    "    max_diffs = torch.max(diffs, dim=-1).values\n",
    "    return summarize(max_diffs, name='rowwise max absolute diffs', include_value=True, linear_fit=True, renderer=renderer)\n",
    "\n",
    "\n",
    "\n",
    "# ## Negligibility of W_pos @ W_V @ W_O @ W_U\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def calculate_OV_of_pos_embed(model: HookedTransformer, renderer=None):\n",
    "    W_U, W_E, W_pos, W_V, W_O = model.W_U, model.W_E, model.W_pos, model.W_V, model.W_O\n",
    "    d_model, d_vocab, n_ctx = model.cfg.d_model, model.cfg.d_vocab, model.cfg.n_ctx\n",
    "    assert W_U.shape == (d_model, d_vocab)\n",
    "    assert W_pos.shape == (n_ctx, d_model)\n",
    "    assert W_E.shape == (d_vocab, d_model)\n",
    "    assert W_V.shape == (1, 1, d_model, d_model)\n",
    "    assert W_O.shape == (1, 1, d_model, d_model)\n",
    "    res = (W_pos @ W_V @ W_O @ W_U).detach()[0,0,:,:]\n",
    "    imshow(res, title='W_pos @ W_V @ W_O @ W_U', xaxis='logit affected', yaxis='position', renderer=renderer)\n",
    "    return summarize(res, name='W_pos @ W_V @ W_O @ W_U', renderer=renderer, linear_fit=True)\n",
    "# %%\n",
    "def analyze_PVOU(model: HookedTransformer, colorscale='RdBu', renderer=None):\n",
    "    W_U, W_E, W_pos, W_V, W_O = model.W_U, model.W_E, model.W_pos, model.W_V, model.W_O\n",
    "    d_model, d_vocab, n_ctx = model.cfg.d_model, model.cfg.d_vocab, model.cfg.n_ctx\n",
    "    assert W_U.shape == (d_model, d_vocab)\n",
    "    assert W_pos.shape == (n_ctx, d_model)\n",
    "    assert W_E.shape == (d_vocab, d_model)\n",
    "    assert W_V.shape == (1, 1, d_model, d_model)\n",
    "    assert W_O.shape == (1, 1, d_model, d_model)\n",
    "    res = (W_pos @ W_V @ W_O @ W_U).detach()[0,0,:,:]\n",
    "    pos_indices = torch.arange(n_ctx)\n",
    "    fig = px.imshow(utils.to_numpy(res), title='W_pos @ W_V @ W_O @ W_U',\n",
    "                    labels={\"x\":'logit affected', \"y\":'position'},\n",
    "                    color_continuous_midpoint=0.0, color_continuous_scale=colorscale)\n",
    "    fig.update_yaxes(tickvals=pos_indices, ticktext=pos_indices)\n",
    "    fig.show(renderer)\n",
    "\n",
    "# %%\n",
    "def analyze_PU(model: HookedTransformer, colorscale='RdBu', renderer=None):\n",
    "    W_U, W_pos = model.W_U, model.W_pos\n",
    "    d_model, d_vocab, n_ctx = model.cfg.d_model, model.cfg.d_vocab, model.cfg.n_ctx\n",
    "    assert W_U.shape == (d_model, d_vocab)\n",
    "    assert W_pos.shape == (n_ctx, d_model)\n",
    "    res = (W_pos[-1, :] @ W_U).detach()\n",
    "    line(res, title='W_pos[-1] @ W_U', xaxis='output token', showlegend=False, hovertemplate='Logit for %{x}: %{y}', renderer=renderer)\n",
    "\n",
    "# %%\n",
    "def analyze_EU(model: HookedTransformer, colorscale='RdBu', renderer=None):\n",
    "    W_U, W_E = model.W_U, model.W_E\n",
    "    d_model, d_vocab = model.cfg.d_model, model.cfg.d_vocab\n",
    "    assert W_U.shape == (d_model, d_vocab)\n",
    "    assert W_E.shape == (d_vocab, d_model)\n",
    "    res = (W_E @ W_U).detach()\n",
    "    imshow(res, title='W_E @ W_U', renderer=renderer,\n",
    "           xaxis=\"logit affected\", yaxis=\"input token\", colorscale=colorscale)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## Copying: W_E @ W_V @ W_O @ W_U\n",
    "\n",
    "# %%\n",
    "def analyze_EVOU(model: HookedTransformer, colorscale='RdBu', renderer=None, scale_by_singular_value=True):\n",
    "    W_U, W_E, W_pos, W_V, W_O = model.W_U, model.W_E, model.W_pos, model.W_V, model.W_O\n",
    "    d_model, d_vocab, n_ctx = model.cfg.d_model, model.cfg.d_vocab, model.cfg.n_ctx\n",
    "    assert W_U.shape == (d_model, d_vocab)\n",
    "    assert W_pos.shape == (n_ctx, d_model)\n",
    "    assert W_E.shape == (d_vocab, d_model)\n",
    "    assert W_V.shape == (1, 1, d_model, d_model)\n",
    "    assert W_O.shape == (1, 1, d_model, d_model)\n",
    "    res = (W_E @ W_V @ W_O @ W_U).detach().cpu()[0,0,:,:]\n",
    "    imshow(res, title='W_E @ W_V @ W_O @ W_U', renderer=renderer,\n",
    "           xaxis=\"logit affected\", yaxis=\"input token\", colorscale=colorscale)\n",
    "    analyze_svd(res, descr='W_E @ W_V @ W_O @ W_U', colorscale=colorscale, scale_by_singular_value=scale_by_singular_value, renderer=renderer)\n",
    "    line(res.diag(), title='(W_E @ W_V @ W_O @ W_U).diag()', xaxis='input token', showlegend=False, hovertemplate='Input Token: %{x}<br>Logit on %{x}: %{y}', renderer=renderer)\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def calculate_copying(model: HookedTransformer, colorscale='RdBu', renderer=None, scale_by_singular_value=True):\n",
    "    W_U, W_E, W_pos, W_V, W_O = model.W_U, model.W_E, model.W_pos, model.W_V, model.W_O\n",
    "    d_model, d_vocab, n_ctx = model.cfg.d_model, model.cfg.d_vocab, model.cfg.n_ctx\n",
    "    assert W_U.shape == (d_model, d_vocab)\n",
    "    assert W_pos.shape == (n_ctx, d_model)\n",
    "    assert W_E.shape == (d_vocab, d_model)\n",
    "    assert W_V.shape == (1, 1, d_model, d_model)\n",
    "    assert W_O.shape == (1, 1, d_model, d_model)\n",
    "    res = (W_E @ W_V @ W_O @ W_U).detach().cpu()[0,0,:,:]\n",
    "    res_diag = res.diag()\n",
    "    res_off_diagonal = res[torch.eye(d_vocab) == 0]\n",
    "    centered = -res + res.diag()[:, None]\n",
    "    nonzero_centered = centered[torch.eye(d_vocab) == 0]\n",
    "    imshow(res, title='W_E @ W_V @ W_O @ W_U', renderer=renderer,\n",
    "           xaxis=\"logit affected\", yaxis=\"input token\")\n",
    "    analyze_svd(res, descr='W_E @ W_V @ W_O @ W_U', colorscale=colorscale, scale_by_singular_value=scale_by_singular_value, renderer=renderer)\n",
    "    # imshow(centered, title='copying.diag()[:,None] - copying', renderer=renderer)\n",
    "    line(res.diag(), title='copying.diag()', xaxis='input token', renderer=renderer)\n",
    "    # take svd of res\n",
    "    u, s, vh = torch.linalg.svd(res)\n",
    "    v = vh.T\n",
    "    # plot singular values\n",
    "    line(s, title='singular values of copying', renderer=renderer)\n",
    "    # plot u, v\n",
    "    imshow(u, title='u', renderer=renderer)\n",
    "    imshow(v, title='v', renderer=renderer)\n",
    "\n",
    "    # 1. We already have u, s, and v from torch.linalg.svd(res)\n",
    "    u1 = u[:, 0]\n",
    "    v1 = v[:, 0]\n",
    "\n",
    "    # 2. Fit linear models to u1 and v1\n",
    "    # Fit for u's first column\n",
    "    x_vals_u = np.arange(d_vocab)\n",
    "    y_vals_u = u[:, 0].numpy()\n",
    "    popt_u, _ = curve_fit(linear_func, x_vals_u, y_vals_u)\n",
    "\n",
    "    # Fit for v's first column\n",
    "    x_vals_v = np.arange(d_vocab)\n",
    "    y_vals_v = v[0, :].numpy()\n",
    "    popt_v, _ = curve_fit(linear_func, x_vals_v, y_vals_v)\n",
    "\n",
    "    # Plot u's column against its linear fit\n",
    "    plt.figure()\n",
    "    plt.scatter(x_vals_u, y_vals_u, alpha=0.5, label='Data')\n",
    "    plt.plot(x_vals_u, linear_func(x_vals_u, *popt_u), 'r-', label=f'u: y = {popt_u[0]:.4f}x + {popt_u[1]:.4f}')\n",
    "    plt.title(\"First Column of u vs Linear Fit\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot residuals for u\n",
    "    plt.figure()\n",
    "    residuals_u = y_vals_u - linear_func(x_vals_u, *popt_u)\n",
    "    plt.scatter(x_vals_u, residuals_u, alpha=0.5)\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.title(\"Residuals of u's First Column Fit\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot v's row against its linear fit\n",
    "    plt.figure()\n",
    "    plt.scatter(x_vals_v, y_vals_v, alpha=0.5, label='Data')\n",
    "    plt.plot(x_vals_v, linear_func(x_vals_v, *popt_v), 'r-', label=f'v: y = {popt_v[0]:.4f}x + {popt_v[1]:.4f}')\n",
    "    plt.title(\"First Row of v vs Linear Fit\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot residuals for v\n",
    "    plt.figure()\n",
    "    residuals_v = y_vals_v - linear_func(x_vals_v, *popt_v)\n",
    "    plt.scatter(x_vals_v, residuals_v, alpha=0.5)\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.title(\"Residuals of v's First Row Fit\")\n",
    "    plt.show()\n",
    "\n",
    "    # Subtract impact of lines\n",
    "    u_prime = linear_func(x_vals_u, *popt_u)\n",
    "    v_prime = linear_func(x_vals_v, *popt_v)\n",
    "    impact = s[0] * u_prime[:, None] @ v_prime[None, :]\n",
    "    adjusted_res = res - impact\n",
    "    imshow(impact, title=\"adjustment\", renderer=renderer)\n",
    "\n",
    "    # adjusted_res = res - s[0] * (u[:, 0:1] @ v[:,0:1].T) * (popt_u[0] * x_vals_u[:, None] + popt_v[0] * x_vals_v[None, :])\n",
    "\n",
    "    imshow(adjusted_res, title='Adjusted res', renderer=renderer)\n",
    "\n",
    "    # SVD of adjusted_res\n",
    "    u_adj, s_adj, vh_adj = torch.linalg.svd(adjusted_res)\n",
    "    line(s_adj, title='Singular Values of Adjusted res', renderer=renderer)\n",
    "    imshow(u_adj, title='u of residuals', renderer=renderer)\n",
    "    imshow(vh_adj.T, title='v of residuals', renderer=renderer)\n",
    "\n",
    "    # Extracting diagonal and off-diagonal entries\n",
    "    diagonal_entries = torch.diag(adjusted_res)\n",
    "    off_diagonal_entries = adjusted_res - torch.diag_embed(diagonal_entries)\n",
    "    off_diagonal_entries = off_diagonal_entries[off_diagonal_entries != 0]\n",
    "\n",
    "    # Finding the smallest diagonal entry and the largest off-diagonal entry\n",
    "    min_diagonal_entry = diagonal_entries.min().item()\n",
    "    max_off_diagonal_entry = off_diagonal_entries.max().item()\n",
    "\n",
    "    # Printing the results\n",
    "    print(f\"Smallest diagonal entry: {min_diagonal_entry} ({pm_range(diagonal_entries)})\")\n",
    "    print(f\"Largest off-diagonal entry: {max_off_diagonal_entry} ({pm_range(off_diagonal_entries)})\")\n",
    "\n",
    "    line(diagonal_entries, title='Diagonal Entries', renderer=renderer)\n",
    "\n",
    "    off_diagonal_entries = off_diagonal_entries.flatten()\n",
    "    # Histogram plot\n",
    "    plt.hist(diagonal_entries.numpy(), bins=50, color='blue', alpha=0.7, label='Diagonal entries')\n",
    "    plt.hist(off_diagonal_entries.numpy(), bins=50, color='red', alpha=0.5, label='Off-diagonal entries')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Histogram of Diagonal and Off-diagonal Entries')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # Histogram plot\n",
    "    plt.hist(diagonal_entries.numpy(), bins=50, color='blue', alpha=0.7, label='Diagonal entries', density=True)\n",
    "    plt.hist(off_diagonal_entries.numpy(), bins=50, color='red', alpha=0.5, label='Off-diagonal entries', density=True)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Density Histogram of Diagonal and Off-diagonal Entries')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    centered_adjusted_res = -adjusted_res + adjusted_res.diag()[:, None]\n",
    "    nonzero_centered_adjusted_res = centered_adjusted_res[centered_adjusted_res != 0]\n",
    "\n",
    "    imshow(centered_adjusted_res, title='adjusted copying.diag()[:,None] - adjusted copying', renderer=renderer)\n",
    "    print(f\"range on nonzero centered adjusted res: {pm_range(nonzero_centered_adjusted_res)}\")\n",
    "\n",
    "    statistics = [\n",
    "        ('copying', res),\n",
    "        ('diag', res_diag),\n",
    "        ('off-diag', res_off_diagonal),\n",
    "        ('centered', centered),\n",
    "        ('nonzero centered', nonzero_centered),\n",
    "    ]\n",
    "\n",
    "    summaries = {name: summarize(value, name=name, renderer=renderer, histogram=False) for name, value in statistics}\n",
    "    for k, v in summaries.items():\n",
    "        print(k, v)\n",
    "    return res\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def calculate_copying_with_pos(model: HookedTransformer, renderer=None):\n",
    "    W_U, W_E, W_pos, W_V, W_O = model.W_U, model.W_E, model.W_pos, model.W_V, model.W_O\n",
    "    d_model, d_vocab, n_ctx = model.cfg.d_model, model.cfg.d_vocab, model.cfg.n_ctx\n",
    "    assert W_U.shape == (d_model, d_vocab)\n",
    "    assert W_pos.shape == (n_ctx, d_model)\n",
    "    assert W_E.shape == (d_vocab, d_model)\n",
    "    assert W_V.shape == (1, 1, d_model, d_model)\n",
    "    assert W_O.shape == (1, 1, d_model, d_model)\n",
    "    res = (W_E @ W_V @ W_O @ W_U).detach()[0,0,:,:]\n",
    "    res_pos = (W_pos @ W_V @ W_O @ W_U).detach()[0,0,:,:]\n",
    "    res_pos_min, res_pos_max = res_pos.min(dim=0).values, res_pos.max(dim=0).values\n",
    "    res_diag = res.diag() + res_pos_min\n",
    "    res_above_diag = -(res + res_pos_max[None,:]) + res_diag[:, None]\n",
    "    imshow(res_above_diag, title='(W_E + worst(W_pos)) @ W_V @ W_O @ W_U', renderer=renderer,\n",
    "              xaxis=\"logit affected\", yaxis=\"input token\")\n",
    "    res_above_diag_off_diag = res_above_diag[torch.eye(d_vocab) == 0]\n",
    "    first_diagonal = res.diag(diagonal=1) + res_pos_min[:-1]\n",
    "    res_above_first_diagonal = -(res[:-1,:] + res_pos_max[None,:]) + first_diagonal[:, None]\n",
    "    statistics = [\n",
    "       ('res_above_diag_off_diag', res_above_diag_off_diag),\n",
    "         ('res_above_first_diagonal', res_above_first_diagonal),\n",
    "    ]\n",
    "    for name, value in statistics:\n",
    "        print(name, summarize(value, name=name, renderer=renderer, histogram=True))\n",
    "\n",
    "\n",
    "# ## Attention Scaling Factor\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def calculate_attn(model: HookedTransformer, pos: Optional[int] = None, renderer=None):\n",
    "    W_U, W_E, W_pos, W_Q, W_K = model.W_U, model.W_E, model.W_pos, model.W_Q, model.W_K\n",
    "    d_model, d_vocab, n_ctx = model.cfg.d_model, model.cfg.d_vocab, model.cfg.n_ctx\n",
    "    if pos is None:\n",
    "        return [calculate_attn(model, pos=i, renderer=renderer) for i in range(n_ctx)]\n",
    "    assert W_U.shape == (d_model, d_vocab)\n",
    "    assert W_pos.shape == (n_ctx, d_model)\n",
    "    assert W_E.shape == (d_vocab, d_model)\n",
    "    assert W_Q.shape == (1, 1, d_model, d_model)\n",
    "    assert W_K.shape == (1, 1, d_model, d_model)\n",
    "    residm1 = (W_E + W_pos[-1,:][None,:])\n",
    "    resid = (W_E + W_pos[pos,:][None,:])\n",
    "    q = (residm1 @ W_Q)[0,0,:,:]\n",
    "    k = (resid @ W_K)[0,0,:,:]\n",
    "    res = (q @ k.T).detach()\n",
    "    # imshow(res, title=f'(W_E + W_pos[-1]) @ W_Q @ W_K.T @ (W_E + W_pos[{pos}]).T', renderer=renderer)\n",
    "    centered = res - res.mean(dim=-1, keepdim=True)\n",
    "    imshow(centered, title=f'centered (W_E + W_pos[-1]) @ W_Q @ W_K.T @ (W_E + W_pos[{pos}]).T', renderer=renderer,\n",
    "           xaxis=\"Key token\", yaxis=\"Query token\")\n",
    "    return centered\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "# check for monotonicity violations\n",
    "def check_monotonicity(model: HookedTransformer, renderer=None):\n",
    "    count = 0\n",
    "    centered_scores = calculate_attn(model, renderer=renderer)\n",
    "    for pos, centered_score in enumerate(centered_scores):\n",
    "        for row_n, row in enumerate(centered_score):\n",
    "            for i in range(row.shape[0] - 1):\n",
    "                for j in range(i + 1, row.shape[0]):\n",
    "                    if row[i] > row[j]:\n",
    "                        count += 1\n",
    "                        print(f\"{i, j} at row {row_n} pos {pos}, magnitude {row[i] - row[j]:.3f}\")\n",
    "    return count\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "def calculate_attn_by_pos(model: HookedTransformer, pos=False, renderer=None):\n",
    "    W_U, W_E, W_pos, W_Q, W_K = model.W_U, model.W_E, model.W_pos, model.W_Q, model.W_K\n",
    "    d_model, d_vocab, n_ctx = model.cfg.d_model, model.cfg.d_vocab, model.cfg.n_ctx\n",
    "    assert W_U.shape == (d_model, d_vocab)\n",
    "    assert W_pos.shape == (n_ctx, d_model)\n",
    "    assert W_E.shape == (d_vocab, d_model)\n",
    "    assert W_Q.shape == (1, 1, d_model, d_model)\n",
    "    assert W_K.shape == (1, 1, d_model, d_model)\n",
    "    residm1 = (W_E + W_pos[-1,:][None,:])\n",
    "\n",
    "    resid = (W_E if not pos else W_pos[0,:][None,:] - W_pos[1, :][None,:])\n",
    "    resid_name = 'W_E' if not pos else f'(W_pos[0] - W_pos[1])'\n",
    "    q = (residm1 @ W_Q)[0,0,:,:]\n",
    "    k = (resid @ W_K)[0,0,:,:]\n",
    "    res = (q @ k.T).detach()\n",
    "    # imshow(res, title=f'(W_E + W_pos[-1]) @ W_Q @ W_K.T @ (W_E + W_pos[{pos}]).T', renderer=renderer)\n",
    "    centered = res - res.mean(dim=-1, keepdim=True) if not pos else res\n",
    "    imshow(centered, title=f'centered (W_E + W_pos[-1]) @ W_Q @ W_K.T @ {resid_name}.T', renderer=renderer,\n",
    "           xaxis=\"Key token\", yaxis=\"Query token\")\n",
    "    #print(centered.shape)\n",
    "    return summarize(centered, name=f'centered (W_E + W_pos[-1]) @ W_Q @ W_K.T @ {resid_name}.T',\n",
    "                     renderer=renderer,\n",
    "                     include_value=True)\n",
    "\n",
    "def replace_nans_with_row_max(tensor):\n",
    "    # Step 1: Identify the nan values\n",
    "    nan_mask = torch.isnan(tensor)\n",
    "\n",
    "    # Step 2: Compute the maximum value for each row, ignoring nans\n",
    "    non_nan_tensor = torch.where(nan_mask, torch.tensor(float('-inf')).to(tensor.device), tensor)\n",
    "    row_max, _ = torch.max(non_nan_tensor, dim=1, keepdim=True)\n",
    "\n",
    "    # Replace nan with the max value of the respective row\n",
    "    tensor[nan_mask] = row_max.expand_as(tensor)[nan_mask]\n",
    "\n",
    "    return tensor\n",
    "\n",
    "def calculate_rowwise_attn_by_pos_near(model: HookedTransformer, pos=False, renderer=None, max_offset=1):\n",
    "    def pad_diagonal_with(shape, diag, offset, val=100000):\n",
    "        before_padding = torch.zeros(list(shape[:-2]) + [np.max([0, -offset])], device=diag.device)\n",
    "        after_padding  = torch.zeros(list(shape[:-2]) + [np.max([0,  offset])], device=diag.device)\n",
    "        before_padding.fill_(val)\n",
    "        after_padding.fill_(val)\n",
    "        return torch.cat([before_padding, diag, after_padding], dim=-1)\n",
    "\n",
    "    points = []\n",
    "    centered_score = calculate_attn_by_pos(model, renderer=renderer, pos=pos)['value']\n",
    "    centered_diag = centered_score.diag()\n",
    "    centered_score = centered_score - centered_diag[:, None]\n",
    "    res = torch.stack([np.sign(offset) * pad_diagonal_with(centered_score.shape, centered_score.diag(diagonal=offset), offset, val=float('nan'))\n",
    "                       for offset in range(-max_offset, max_offset + 1) if offset != 0], dim=-1)\n",
    "    imshow(centered_score, renderer=renderer)\n",
    "    imshow(res, renderer=renderer)\n",
    "    res = replace_nans_with_row_max(res)\n",
    "    min_right_attn = res.min(dim=-1).values\n",
    "    return summarize(min_right_attn, name=f'min right attn by pos near {max_offset}', renderer=renderer, include_value=True, fit_function=quadratic_func)\n",
    "    #return min(points)\n",
    "\n",
    "def calculate_min_attn_by_pos_far(model: HookedTransformer, pos=False, renderer=None, min_offset=2):\n",
    "    points = []\n",
    "    centered_score = calculate_attn_by_pos(model, renderer=renderer, pos=pos)['value']\n",
    "    for row_n, row in enumerate(centered_score):\n",
    "        for i in range(row.shape[0]):\n",
    "            if i != row_n and abs(i - row_n) >= min_offset:\n",
    "                points.append((row[i].item() - row[row_n].item())  / (i - row_n))\n",
    "    # histogram\n",
    "    plt.hist(points, bins=100, edgecolor='black')\n",
    "    return min(points)\n",
    "\n",
    "# ## Attention Patterns\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def calculate_qk_attn_heatmap(model, keypos=-1, querypos=-1, do_layernorm=True):\n",
    "    attn = model.blocks[0].attn\n",
    "    all_token_embeddings = model.embed(range(model.cfg.d_vocab))\n",
    "    positional_embeddings = model.pos_embed(all_token_embeddings)\n",
    "\n",
    "    token_embeddings_at_keypos = all_token_embeddings + positional_embeddings[:,keypos,:] if keypos > -1 else all_token_embeddings\n",
    "    token_embeddings_at_querypos = all_token_embeddings + positional_embeddings[:,querypos,:] if querypos > -1 else all_token_embeddings\n",
    "\n",
    "    # layernorm before attention\n",
    "    if do_layernorm:\n",
    "        token_embeddings_at_keypos = model.blocks[0].ln1(token_embeddings_at_keypos)\n",
    "        token_embeddings_at_querypos = model.blocks[0].ln1(token_embeddings_at_querypos)\n",
    "\n",
    "    embeddings_key = einsum(\"d_vocab d_model, n_heads d_model d_head -> n_heads d_vocab d_head\",\n",
    "                            token_embeddings_at_keypos, attn.W_K)\n",
    "    embeddings_query = einsum(\"d_vocab d_model, n_heads d_model d_head -> n_heads d_vocab d_head\",\n",
    "                            token_embeddings_at_querypos, attn.W_Q)\n",
    "\n",
    "    qk_circuit_attn_heatmap = einsum(\n",
    "        \"n_heads d_vocab_q d_head, n_heads d_vocab_k d_head -> ... d_vocab_q d_vocab_k\",\n",
    "        embeddings_query, embeddings_key\n",
    "        ).detach().cpu().numpy()\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = [20, 10]\n",
    "    return qk_circuit_attn_heatmap\n",
    "\n",
    "\n",
    "def calculate_qk_attn_heatmap_normed(model, querypos=-1, do_layernorm=True, skip_var=True):\n",
    "    all_token_embeddings = model.embed(range(model.cfg.d_vocab))\n",
    "    positional_embeddings = model.pos_embed(all_token_embeddings)\n",
    "    all_heatmaps = torch.stack([torch.tensor(calculate_qk_attn_heatmap(model, cur_keypos, querypos, do_layernorm=do_layernorm)) for cur_keypos in range(positional_embeddings.shape[-2])])\n",
    "    avg = einops.reduce(all_heatmaps, \"keypos d_vocab_q d_vocab_k -> d_vocab_q ()\", 'mean')\n",
    "    var = einops.reduce(all_heatmaps, \"keypos d_vocab_q d_vocab_k -> d_vocab_q ()\", torch.var)\n",
    "    #print(all_heatmaps.shape, avg.shape)\n",
    "    #print(avg)\n",
    "    res = (all_heatmaps - avg)\n",
    "    if not skip_var: res = res * (var ** -0.5)\n",
    "    return res\n",
    "\n",
    "\n",
    "def plot_qk_heatmap(model, keypos=-1, querypos=-1, do_layernorm=True):\n",
    "  qk_attn_heatmap = calculate_qk_attn_heatmap(model, keypos=keypos, querypos=querypos, do_layernorm=do_layernorm)\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(8, 8))\n",
    "  graph = ax.imshow(qk_attn_heatmap, cmap=\"hot\", interpolation=\"nearest\")\n",
    "  plt.colorbar(graph)\n",
    "  plt.tight_layout()\n",
    "\n",
    "\n",
    "def plot_qk_heatmaps_normed(model, keypositions=None, querypos=-1, do_layernorm=True, skip_var=True):\n",
    "    if keypositions is None:\n",
    "        all_token_embeddings = model.embed(range(model.cfg.d_vocab))\n",
    "        positional_embeddings = model.pos_embed(all_token_embeddings)\n",
    "        keypositions = range(positional_embeddings.shape[-2])\n",
    "\n",
    "    heatmaps = calculate_qk_attn_heatmap_normed(model, querypos=querypos, do_layernorm=do_layernorm, skip_var=skip_var)\n",
    "    for keypos in keypositions:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        qk_attn_heatmap = heatmaps[keypos]\n",
    "        graph = ax.imshow(qk_attn_heatmap, cmap=\"hot\", interpolation=\"nearest\")\n",
    "        plt.colorbar(graph)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    print(heatmaps.shape) # torch.Size([2, 64, 64]), keypos d_vocab_q d_vocab_k\n",
    "\n",
    "\n",
    "def plot_avg_qk_heatmap(model, keypositions, querypos=-1, do_layernorm=True):\n",
    "  heatmaps = []\n",
    "\n",
    "  for keypos in keypositions:\n",
    "    heatmaps.append(calculate_qk_attn_heatmap(model, keypos=keypos, querypos=querypos, do_layernorm=do_layernorm))\n",
    "\n",
    "  qk_circuit_attn_heatmap = np.mean(heatmaps, axis=0)\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(8, 8))\n",
    "  graph = ax.imshow(qk_circuit_attn_heatmap, cmap=\"hot\", interpolation=\"nearest\")\n",
    "  plt.colorbar(graph)\n",
    "  plt.tight_layout()\n",
    "\n",
    "\n",
    "#list(zip(all_integers[~correct_idxs], all_integers_ans[~correct_idxs]))\n",
    "\n",
    "\n",
    "# # Interpretability\n",
    "\n",
    "# ## Unembed\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def plot_unembed_cosine_similarity(model):\n",
    "    all_token_embeddings = model.embed(range(model.cfg.d_vocab))\n",
    "    positional_embeddings = model.pos_embed(all_token_embeddings)\n",
    "    all_token_pos_embed = all_token_embeddings[:,None,:] + positional_embeddings\n",
    "    #print(model.W_U.shape, all_token_embeddings.shape, positional_embeddings.shape)\n",
    "    # torch.Size([32, 64]) torch.Size([64, 32]) torch.Size([64, 2, 32])\n",
    "    avg = F.normalize(all_token_embeddings.sum(dim=0), dim=-1)\n",
    "    # overlap between model.W_U and token embedings\n",
    "    input_overlap = all_token_pos_embed @ model.W_U\n",
    "    print(f\"Definition max_input_output_overlap := {input_overlap.abs().max()}.\")\n",
    "    line(F.cosine_similarity(avg[None,:], all_token_embeddings, dim=-1))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def count_monotonicity_violations_line(result_tensor, m):\n",
    "    # Count the number of pairs of indices (i, j), i != j, for which\n",
    "    # (result_tensor[i] + m*i - result_tensor[j] + m*j) / (i - j) is negative\n",
    "    count = 0\n",
    "    for i in range(len(result_tensor)):\n",
    "        for j in range(i + 1, len(result_tensor)):\n",
    "            if ((result_tensor[i] + m*i - result_tensor[j] + m*j) / (i - j)) < 0:\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def reorder_tensor_greedy(tensor, m):\n",
    "    # Convert to numpy for easier handling\n",
    "    tensor_np = tensor.detach().clone().numpy()\n",
    "\n",
    "    # Initialize the result with the maximum positive value\n",
    "    result = [np.max(tensor_np)]\n",
    "    tensor_np = np.delete(tensor_np, np.argmax(tensor_np))\n",
    "\n",
    "    while len(tensor_np) > 0:\n",
    "        # Find values that maintain the condition\n",
    "        candidates = tensor_np[tensor_np - result[-1] < -m]\n",
    "\n",
    "        if len(candidates) > 0:\n",
    "            # If such values exist, select the maximum\n",
    "            next_value = np.max(candidates)\n",
    "        else:\n",
    "            # Otherwise, select the maximum of the remaining values\n",
    "            next_value = np.max(tensor_np)\n",
    "\n",
    "        # Add the selected value to the result\n",
    "        result.append(next_value)\n",
    "\n",
    "        # Remove the selected value from the list of remaining values\n",
    "        tensor_np = np.delete(tensor_np, np.where(tensor_np == next_value)[0][0])\n",
    "\n",
    "    # Convert the result back to a tensor\n",
    "    result_tensor = torch.tensor(result)\n",
    "\n",
    "    # Count the number of indices for which the difference between\n",
    "    # successive elements in the result is less than -m\n",
    "    # diff = result_tensor[1:] - result_tensor[:-1]\n",
    "    # count = torch.sum(diff < -m).item()\n",
    "\n",
    "    count = count_monotonicity_violations_line(result_tensor, m)\n",
    "\n",
    "    return result_tensor, count\n",
    "\n",
    "\n",
    "def compute_best_fit_and_error(direction_dot_embed):\n",
    "    n_head, d_vocab = direction_dot_embed.shape\n",
    "\n",
    "    coefficients = torch.empty((n_head, 2))  # To store the coefficients a, b for each row\n",
    "    max_abs_errors = torch.empty(n_head)  # To store the max abs error for each row\n",
    "    errors = torch.empty((n_head, d_vocab))\n",
    "    predicted = torch.empty((n_head, d_vocab))\n",
    "    negative_pairs = []\n",
    "    diff_values = []\n",
    "\n",
    "    x_values = np.arange(d_vocab)\n",
    "\n",
    "    # Create a meshgrid of indices\n",
    "    idxi, idxj = np.meshgrid(x_values, x_values)\n",
    "    # Exclude the diagonal (i == j)\n",
    "    mask = idxi != idxj\n",
    "    pairs = list(zip(idxi[mask], idxj[mask]))  # create a list of pairs (i, j)\n",
    "\n",
    "    for i in range(n_head):\n",
    "        row = direction_dot_embed[i].detach().numpy()\n",
    "\n",
    "        # Use curve_fit to find a, b that best fit the data in this row\n",
    "        coeff, _ = curve_fit(linear_func, x_values, row)\n",
    "        coefficients[i] = torch.from_numpy(coeff)\n",
    "\n",
    "        # Compute the predicted y values using these coefficients\n",
    "        y_pred = coeff[0] * x_values + coeff[1]\n",
    "\n",
    "        # Compute the absolute error for each value, and take the maximum\n",
    "        cur_errors = row - y_pred\n",
    "        max_abs_errors[i] = np.abs(cur_errors).max()\n",
    "        errors[i] = torch.from_numpy(cur_errors)\n",
    "        predicted[i] = torch.from_numpy(y_pred)\n",
    "\n",
    "        # Compute (pos[i] - pos[j]) / (i - j) for all pairs (i, j)\n",
    "        values = (row[idxi] - row[idxj]) / (idxi - idxj)\n",
    "\n",
    "        # Select only the values where i != j\n",
    "        values = values[mask]\n",
    "        negative_pairs.append([pair for pair, value in zip(pairs, values) if value < 0])\n",
    "\n",
    "        diff_values.append(values)\n",
    "\n",
    "    return coefficients, max_abs_errors, errors, predicted, diff_values, negative_pairs\n",
    "\n",
    "\n",
    "def plot_QK_cosine_similarity(model, keypos=-1, querypos=-1, do_layernorm=True):\n",
    "    attn = model.blocks[0].attn\n",
    "    all_token_embeddings = model.embed(range(model.cfg.d_vocab))\n",
    "    positional_embeddings = model.pos_embed(all_token_embeddings)\n",
    "    normed_all_token_embeddings = F.normalize(all_token_embeddings, dim=-1)\n",
    "\n",
    "    token_embeddings_at_keypos = all_token_embeddings + positional_embeddings[:,keypos,:] if keypos > -1 else all_token_embeddings\n",
    "    token_embeddings_at_querypos = all_token_embeddings + positional_embeddings[:,querypos,:] if querypos > -1 else all_token_embeddings\n",
    "\n",
    "    # layernorm before attention\n",
    "    if do_layernorm:\n",
    "        token_embeddings_at_keypos = model.blocks[0].ln1(token_embeddings_at_keypos)\n",
    "        token_embeddings_at_querypos = model.blocks[0].ln1(token_embeddings_at_querypos)\n",
    "\n",
    "    #embeddings_key = einsum(\"d_vocab d_model, n_heads d_model d_head -> n_heads d_vocab d_head\",\n",
    "    #                        token_embeddings_at_keypos, attn.W_K)\n",
    "    #embeddings_query = einsum(\"d_vocab d_model, n_heads d_model d_head -> n_heads d_vocab d_head\",\n",
    "    #                        token_embeddings_at_querypos, attn.W_Q)\n",
    "    embeddings_query_waiting_for_key = einsum(\"d_vocab_query d_model_query, n_heads d_model_query d_head, n_heads d_model_key d_head -> n_heads d_vocab_query d_model_key\",\n",
    "                            token_embeddings_at_querypos, attn.W_Q, attn.W_K)\n",
    "\n",
    "    QK = einsum(\"n_heads d_model_query d_head, n_heads d_model_key d_head -> n_heads d_model_query d_model_key\",\n",
    "                            attn.W_Q, attn.W_K)\n",
    "\n",
    "    analyze_svd(embeddings_query_waiting_for_key[0], descr=\"embeddings_query_waiting_for_key\")\n",
    "    analyze_svd(QK[0], descr=\"QK\")\n",
    "    U, S, Vh = torch.linalg.svd(embeddings_query_waiting_for_key[0])\n",
    "    print(Vh.T[0])\n",
    "    print(Vh[0])\n",
    "    print((U @ torch.diag(S) @ Vh)[0])\n",
    "    print((U @ torch.diag(S) @ Vh).T[0])\n",
    "    imshow(U @ torch.diag(S) @ Vh, title=\"tmp\")\n",
    "    #qk_circuit_attn_heatmap = einsum(\n",
    "    #    \"n_heads d_vocab_q d_head, n_heads d_vocab_k d_head -> ... d_vocab_q d_vocab_k\",\n",
    "    #    embeddings_query, embeddings_key\n",
    "    #    ).detach().cpu().numpy()\n",
    "\n",
    "    imshow(embeddings_query_waiting_for_key[0])\n",
    "\n",
    "\n",
    "    direction = embeddings_query_waiting_for_key\n",
    "    #direction = direction / direction.norm(dim=-1, keepdim=True)\n",
    "    direction = direction.sum(dim=1) / direction.shape[1]\n",
    "    print(f\"Definition size_direction := {direction}.\")\n",
    "    direction = direction / direction.norm(dim=-1)\n",
    "    print(f\"Definition normed_size_direction := {direction}.\")\n",
    "    print(all_token_embeddings.shape, direction.shape)\n",
    "    proj_direction_scale = einsum(\"n_head d_model_key, n_head d_vocab_query d_model_key -> n_head d_vocab_query\",\n",
    "                                  direction,\n",
    "                                  embeddings_query_waiting_for_key)[:,:,None]\n",
    "    print(proj_direction_scale.shape)\n",
    "    proj_direction = proj_direction_scale * einops.rearrange(direction, \"n_head d_model -> n_head () d_model\")\n",
    "    print(proj_direction.shape)\n",
    "    remaining_directions = embeddings_query_waiting_for_key - proj_direction\n",
    "    print(remaining_directions.shape)\n",
    "    remaining_directions = remaining_directions.norm(dim=-1)\n",
    "    print(remaining_directions.shape)\n",
    "    direction_key_overlap = einsum(\"n_head d_model_key, n_head d_vocab_query d_model_key -> d_vocab_query n_head\",\n",
    "                direction,\n",
    "                embeddings_query_waiting_for_key)\n",
    "    print(direction_key_overlap.shape)\n",
    "    print(f\"Definition min_attention_query_size_direction_overlap := {direction_key_overlap.min()}.\")\n",
    "    direction_dot_embed = einsum(\"n_head d_model, d_vocab d_model -> n_head d_vocab\", direction, normed_all_token_embeddings)\n",
    "    direction_dot_pos_embed = einsum(\"n_head d_model, pos d_model -> n_head pos\", direction, positional_embeddings[0])\n",
    "    print(f\"Definition max_direction_dot_pos_embed := {direction_dot_pos_embed.abs().max()}.\")\n",
    "    # linear fit of direction_dot_embed\n",
    "    direction_dot_embed_coefficients, direction_dot_embed_max_abs_errors, direction_dot_embed_error, direction_dot_embed_predicted, direction_dot_embed_diff_values, direction_dot_embed_neg_values = \\\n",
    "          compute_best_fit_and_error(direction_dot_embed)\n",
    "\n",
    "    direction_dot_embed_diffs = direction_dot_embed[...,1:] - direction_dot_embed[...,:-1]\n",
    "    #direction_dot_embed_coef = direction_dot_embed_diffs.mean(dim=-1, keepdim=True)\n",
    "    #direction_dot_embed_offset = direction_dot_embed.mean(dim=-1, keepdim=True)\n",
    "    #direction_dot_embed_diff_error = direction_dot_embed_diffs - torch.arange(direction_dot_embed_diffs.shape[-1]) * direction_dot_embed_coef + direction_dot_embed_offset)\n",
    "    print(direction_dot_embed_diffs)\n",
    "    print(direction_dot_embed_diffs.abs())\n",
    "    line(direction_dot_embed_diffs.T, title=\"direction_dot_embed_diffs\")\n",
    "    line(direction_dot_embed_diffs.T.abs(), title=\"direction_dot_embed_diffs abs\")\n",
    "    #line(direction_dot_embed_diff_error.T, title=\"direction_dot_embed_diff_error\")\n",
    "    #print(direction_dot_embed_coef, direction_dot_embed_offset)\n",
    "\n",
    "\n",
    "    #direction_dot_embed_coef_better, _ = curve_fit(constant_function, np.arange(direction_dot_embed_diffs.shape[-1]), direction_dot_embed_diffs[0].detach().numpy())\n",
    "    #direction_dot_embed_diff_error_better = direction_dot_embed_diffs - (torch.arange(direction_dot_embed_diffs.shape[-1]) * direction_dot_embed_coef + direction_dot_embed_offset)\n",
    "    #line(direction_dot_embed_diffs.T, title=\"direction_dot_embed_diffs\")\n",
    "    #line(direction_dot_embed_diff_error.T, title=\"direction_dot_embed_diff_error\")\n",
    "    #print(direction_dot_embed_coef, direction_dot_embed_offset)\n",
    "\n",
    "\n",
    "    # indices = np.argsort(direction_dot_embed_error[0].numpy() / np.arange(1, len(direction_dot_embed_error[0]) + 1))\n",
    "\n",
    "    # Use these indices to sort 'direction_dot_embed_error'\n",
    "    # sorted_direction_dot_embed_error = direction_dot_embed_error[:,indices]\n",
    "    print(direction_dot_embed_error.mean(), direction_dot_embed_error.var())\n",
    "    # randomly reorder direction_dot_embed_error, put in tmp\n",
    "    tmp = direction_dot_embed_error[0].detach().clone().numpy()\n",
    "    np.random.shuffle(tmp)\n",
    "    print(tmp)\n",
    "    line(tmp)\n",
    "    print(count_monotonicity_violations_line(torch.tensor(tmp), direction_dot_embed_coefficients[0, 0].item()))\n",
    "    print(f\"Definition \")\n",
    "    sorted_direction_dot_embed_error, bad_count = reorder_tensor_greedy(direction_dot_embed_error[0], direction_dot_embed_coefficients[0, 0].item())\n",
    "    sorted_direction_dot_embed_error = sorted_direction_dot_embed_error[None,:]\n",
    "\n",
    "    # sorted_direction_dot_embed_error, _ = direction_dot_embed_error.sort(dim=-1, descending=True)\n",
    "    print(direction_dot_embed_coefficients, direction_dot_embed_max_abs_errors, direction_dot_embed)\n",
    "    line(direction_key_overlap, title=\"direction @ query_waiting_for_key\")\n",
    "    line(remaining_directions.T, title=\"norm of remaining direction\")\n",
    "    line(F.cosine_similarity(direction, all_token_embeddings, dim=-1), title=\"cos_sim(direction, embed)\")\n",
    "    print(positional_embeddings.shape)\n",
    "    line(direction_dot_embed.T, title=\"direction @ normed embed\")\n",
    "    line(torch.cat([direction_dot_embed, direction_dot_embed_predicted], dim=0).T, title=\"direction @ normed embed, + fit\")\n",
    "    print(bad_count)\n",
    "    line(torch.cat([direction_dot_embed_predicted, direction_dot_embed_predicted + sorted_direction_dot_embed_error], dim=0).T, title=\"direction @ normed embed bad fit\")\n",
    "\n",
    "    # Plot the histogram\n",
    "    print(len(direction_dot_embed_neg_values[0]) // 2)\n",
    "    print(list(sorted([p for p in direction_dot_embed_neg_values[0] if p[0] < p[1]])))\n",
    "    plt.hist(direction_dot_embed_diff_values[0], bins=30, edgecolor='black')\n",
    "    plt.title(\"Distribution of (pos[i] - pos[j]) / (i - j)\")\n",
    "    plt.xlabel(\"(pos[i] - pos[j]) / (i - j)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    line(direction_dot_embed_error.T, title=\"direction_dot_normed_embed_error\")\n",
    "    line(direction_dot_pos_embed.T, title=\"direction @ pos_embed\")\n",
    "\n",
    "\n",
    "# %%\n",
    "def make_local_tqdm(tqdm):\n",
    "    if tqdm is None:\n",
    "        return lambda arg, **kwargs: arg\n",
    "    else:\n",
    "        return tqdm\n",
    "\n",
    "# %%\n",
    "@torch.no_grad()\n",
    "def layernorm_noscale(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x - x.mean(axis=-1, keepdim=True)\n",
    "\n",
    "# %%\n",
    "@torch.no_grad()\n",
    "def layernorm_scales(x: torch.Tensor, eps: float = 1e-5, recip: bool = True) -> torch.Tensor:\n",
    "    x = layernorm_noscale(x)\n",
    "    scale = (x.pow(2).mean(axis=-1, keepdim=True) + eps).sqrt()\n",
    "    if recip: scale = 1 / scale\n",
    "    return scale\n",
    "\n",
    "# %%\n",
    "@torch.no_grad()\n",
    "def compute_singular_contribution(M: torch.Tensor, plot_heatmaps=True, yaxis=None, xaxis=None, title=None, renderer=None, description=None, singular_value_count=1, **kwargs) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    U, S, Vh = torch.linalg.svd(M)\n",
    "    U[:, singular_value_count:], S[singular_value_count:], Vh[singular_value_count:, :] = 0, 0, 0\n",
    "    contribution = U @ torch.diag(S) @ Vh\n",
    "    if plot_heatmaps:\n",
    "        singular_value_str = f\"first {singular_value_count} singular values\" if singular_value_count != 1 else f\"first singular value\"\n",
    "        to_description = f\" to {description}\" if description is not None else \"\"\n",
    "        description = f\"{description} \" if description is not None else \"\"\n",
    "        diff_zmax = (M - contribution).abs().max().item()\n",
    "        zmax = np.max([contribution.abs().max().item(), diff_zmax])\n",
    "        fig = make_subplots(rows=1, cols=3, subplot_titles=[f\"Contribution\", f\"Residual\", f\"Residual (rescaled)\"])\n",
    "        fig.add_trace(go.Heatmap(z=utils.to_numpy(contribution), zmin=-zmax, zmax=zmax, showscale=True,\n",
    "                                 colorbar=dict(x=-0.15, y=0.5),\n",
    "                                **kwargs),\n",
    "                    row=1, col=1)\n",
    "        fig.add_trace(go.Heatmap(z=utils.to_numpy(M - contribution), zmin=-zmax, zmax=zmax, showscale=False,\n",
    "                                **kwargs),\n",
    "                    row=1, col=2)\n",
    "        fig.add_trace(go.Heatmap(z=utils.to_numpy(M - contribution), zmin=-diff_zmax, zmax=diff_zmax, showscale=True,\n",
    "                                **kwargs),\n",
    "                    row=1, col=3)\n",
    "        if title is None: title = f\"Contribution of the {singular_value_str}{to_description}\"\n",
    "        fig.update_layout(title=title, margin=dict(l=100))\n",
    "        for col in range(3):\n",
    "            if yaxis is not None: fig.update_yaxes(title_text=yaxis, row=1, col=col+1)\n",
    "            if xaxis is not None: fig.update_xaxes(title_text=xaxis, row=1, col=col+1)\n",
    "    fig.show(renderer)\n",
    "    return M - contribution, contribution\n",
    "# %%\n",
    "\n",
    "def display_size_direction_stats(size_direction: torch.Tensor, query_direction: torch.Tensor, QK: torch.Tensor, U: torch.Tensor, Vh: torch.Tensor, S: torch.Tensor,\n",
    "                                 size_direction_resid: Optional[torch.Tensor] = None, size_direction_QK: Optional[torch.Tensor] = None,\n",
    "                                 query_direction_resid: Optional[torch.Tensor] = None, query_direction_QK: Optional[torch.Tensor] = None,\n",
    "                                 do_exclusions: bool = True,\n",
    "                                 include_contribution: bool = True,\n",
    "                                 scale_by_singular_value: bool = True,\n",
    "                                 renderer=None,\n",
    "                                 fit_funcs: Iterable = (cubic_func, quintic_func),\n",
    "                                 delta_fit_funcs: Iterable = (quadratic_func, quartic_func),\n",
    "                                 colorscale='Plasma_r', **kwargs):\n",
    "    if scale_by_singular_value:\n",
    "        U = U * S[None, :].sqrt()\n",
    "        Vh = Vh * S[:, None].sqrt()\n",
    "    imshow(QK, title=\"Attention<br>(W_E + W_pos[-1]) @ W_Q @ W_K.T @ (W_E + W_pos.mean(dim=0)).T\", xaxis=\"Key Token\", yaxis=\"Query Token\", renderer=renderer, colorscale=colorscale, **kwargs)\n",
    "    fig = make_subplots(rows=1, cols=3, subplot_titles=[\"Query-Side SVD\", \"Singular Values\", \"Key-Side SVD\"])\n",
    "    uzmax, vzmax = U.abs().max().item(), Vh.abs().max().item()\n",
    "    fig.add_trace(go.Heatmap(z=utils.to_numpy(U), colorscale=colorscale, zmin=-uzmax, zmax=uzmax,\n",
    "                             showscale=False,\n",
    "                            #  colorbar=dict(x=-0.15, # https://community.plotly.com/t/colorbar-ticks-left-aligned/60473/4\n",
    "                            #             ticklabelposition='inside',\n",
    "                            #             ticksuffix='     ',\n",
    "                            #             ticklabeloverflow='allow',\n",
    "                            #             tickfont_color='darkslategrey',),\n",
    "                            hovertemplate=\"Query: %{y}<br>Singular Index: %{x}<br>Value: %{z}<extra></extra>\",\n",
    "                            ),\n",
    "                row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=np.arange(S.shape[0]), y=utils.to_numpy(S),\n",
    "                            mode='lines+markers',\n",
    "                            marker=dict(color='blue'),\n",
    "                            line=dict(color='blue'),\n",
    "                            hovertemplate=\"Singular Value: %{y}<br>Singular Index: %{x}<extra></extra>\",\n",
    "                            ), row=1, col=2)\n",
    "    fig.add_trace(go.Heatmap(z=utils.to_numpy(Vh.T), colorscale=colorscale, zmin=-vzmax, zmax=vzmax,\n",
    "                             showscale=False,\n",
    "                            #  colorbar=dict(x=1.15),\n",
    "                            hovertemplate=\"Key: %{y}<br>Singular Index: %{x}<br>Value: %{z}<extra></extra>\",\n",
    "                            ),\n",
    "                row=1, col=3)\n",
    "    fig.update_layout(title=\"Attention SVD\") #, margin=dict(l=150, r=150))\n",
    "    fig.update_yaxes(title_text=\"Query Token\", row=1, col=1)\n",
    "    fig.update_yaxes(range=[0, None], row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Key Token\", row=1, col=3)\n",
    "    fig.show(renderer)\n",
    "\n",
    "    contribution_diff = None\n",
    "    if include_contribution:\n",
    "        contribution_diff, _ = compute_singular_contribution(\n",
    "            QK, description=\"Attention\", colorscale=colorscale, renderer=renderer, singular_value_count=1,\n",
    "            xaxis='Key Token', yaxis='Query Token',\n",
    "            hovertemplate=\"Query: %{y}<br>Key: %{x}<br>Value: %{z}<extra></extra>\",\n",
    "            **kwargs)\n",
    "\n",
    "    # imshow(U, title=\"Query-Side SVD\", yaxis=\"Query Token\", renderer=renderer, **kwargs)\n",
    "    # imshow(Vh.T, title=\"Key-Side SVD\", yaxis=\"Key Token\", renderer=renderer, **kwargs)\n",
    "    # px.line({'singular values': utils.to_numpy(S)}, title=\"Singular Values of QK Attention\").show(renderer)\n",
    "\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Size\", \"Query\"])\n",
    "    fig.add_trace(go.Scatter(x=np.arange(size_direction.shape[0]), y=utils.to_numpy(size_direction),\n",
    "                            mode='lines+markers',\n",
    "                            marker=dict(color='blue'),\n",
    "                            line=dict(color='blue'),\n",
    "                            hovertemplate=\"Token: %{x}<br>Size: %{y}<extra></extra>\",\n",
    "                            ), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=np.arange(query_direction.shape[0]), y=utils.to_numpy(query_direction),\n",
    "                            mode='lines+markers',\n",
    "                            marker=dict(color='blue'),\n",
    "                            line=dict(color='blue'),\n",
    "                            hovertemplate=\"Token: %{x}<br>Query Value: %{y}<extra></extra>\",\n",
    "                            ), row=1, col=2)\n",
    "    fig.update_layout(title=\"Directions in Token Space\", showlegend=False)\n",
    "    fig.show(renderer)\n",
    "\n",
    "    # px.line({'size direction': utils.to_numpy(size_direction)}, title=\"size direction in token space\").show(renderer)\n",
    "    # px.line({'query direction': utils.to_numpy(query_direction)}, title=\"query direction in token space\").show(renderer)\n",
    "    if size_direction_resid is not None: line(size_direction_resid, title=\"size direction in residual space\", renderer=renderer)\n",
    "    if query_direction_resid is not None: line(query_direction_resid, title=\"query direction in residual space\", renderer=renderer)\n",
    "    if size_direction_QK is not None: line(size_direction_QK, title=\"size direction in QK space\", renderer=renderer)\n",
    "    if query_direction_QK is not None: line(query_direction_QK, title=\"query direction in QK space\", renderer=renderer)\n",
    "\n",
    "    reference_lines = []\n",
    "    if contribution_diff is not None:\n",
    "        # we make some reference lines for the plots of size[i+1] - size[i]\n",
    "        # since we'll eventually multiply these by the singular value and the query direction entry, we want to divide by this product when comparing to values from the non-size-direction contributions\n",
    "        # we compute the mean and worst-case behavior, and a more fine-grained worst-case adjacent difference\n",
    "        singular_scale = S[0].item()\n",
    "        scale_per_query = query_direction * singular_scale\n",
    "        resid_diffs = contribution_diff[:, :-1] - contribution_diff[:, 1:]\n",
    "        resid_max_diff = contribution_diff.max().item() - contribution_diff.min().item()\n",
    "        resid_max_diff_per_query = contribution_diff.max(dim=1).values - contribution_diff.min(dim=1).values\n",
    "        scale_mean, scale_min = scale_per_query.mean(dim=0).item(), scale_per_query.min().item()\n",
    "        resid_mean_diff = (contribution_diff[:, :, None, None] - contribution_diff[None, None, :, :]).abs().mean().item()\n",
    "        resid_mean_diff_per_query = (contribution_diff[:, :, None] - contribution_diff[:, None, :]).abs().mean(dim=(-2, -1))\n",
    "        reference_lines = [\n",
    "            (\"resid.max - resid.min (worst-case independent query)\", resid_max_diff / scale_min),\n",
    "            (\"resid.max - resid.min (average-case independent query)\", resid_max_diff / scale_mean),\n",
    "            (\"resid.max - resid.min (worst-case query)\", (resid_max_diff_per_query / scale_per_query).max().item()),\n",
    "            (\"(resid[i] - resid[i+1]).max (worst-case independent query)\", (resid_diffs / scale_min).max().item()),\n",
    "            (\"(resid[i] - resid[i+1]).max (worst-case query)\", (resid_diffs / scale_per_query[:, None]).max().item()),\n",
    "            (\"(resid[i] - resid[i+1]).abs.mean (average-case independent query)\", (resid_diffs / scale_mean).abs().mean().item()),\n",
    "            (\"(resid[i] - resid[j]).abs.mean (average-case independent query)\", resid_mean_diff / scale_mean),\n",
    "            (\"(resid[i] - resid[j]).abs.mean (average-case query)\", (resid_mean_diff_per_query / scale_per_query).abs().mean().item()),\n",
    "        ]\n",
    "\n",
    "    size_direction_differences = size_direction[1:] - size_direction[:-1]\n",
    "    show_fits(size_direction, name='Size Direction', fit_funcs=(fit_func for fit_func in fit_funcs if fit_func is not sigmoid_func),\n",
    "              do_exclusions=do_exclusions, renderer=renderer)\n",
    "    show_fits(size_direction_differences, name='Size Direction Δ', reference_lines=reference_lines, fit_funcs=(fit_func for fit_func in delta_fit_funcs if fit_func is not sigmoid_func),\n",
    "              do_exclusions=do_exclusions, renderer=renderer)\n",
    "\n",
    "    y_data = size_direction.detach().cpu().numpy()\n",
    "    x_data = np.linspace(1, len(y_data), len(y_data))\n",
    "\n",
    "    for fit_func in fit_funcs:\n",
    "        fit_func_name = fit_func.__name__\n",
    "        if fit_func_name.endswith(\"_func\"): fit_func_name = fit_func_name[:-len(\"_func\")]\n",
    "\n",
    "        if fit_func is sigmoid_func:\n",
    "            # fit to sigmoid\n",
    "            y_transposed = np.linspace(1, len(x_data), len(x_data))\n",
    "            initial_params_transposed = [max(y_transposed), 1/np.mean(y_data), np.median(y_data)]\n",
    "\n",
    "            # Fit the curve with initial parameters\n",
    "\n",
    "            params_transposed, covariance_transposed = curve_fit(sigmoid_func, y_data, y_transposed, p0=initial_params_transposed, maxfev=10000)\n",
    "\n",
    "            # Generate predicted y values with parameters\n",
    "            y_pred_transposed = sigmoid_func(y_data, *params_transposed)\n",
    "            # Calculating residuals\n",
    "            residuals = y_transposed - y_pred_transposed\n",
    "\n",
    "            # Creating subplots\n",
    "            fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "            fig.suptitle('Fitting a Sigmoid to the Size Vector Components and Residuals Analysis', fontsize=16)\n",
    "\n",
    "            # Plotting the original data and fitted curve\n",
    "            axs[0].scatter(y_data, y_transposed, label='Data', color='blue')\n",
    "            axs[0].plot(y_data, y_pred_transposed, color='red',\n",
    "                    label=rf'{inv_sigmoid_func.equation(params_transposed)}')\n",
    "            axs[0].set_xlabel('Component in Normalized Size Vector')\n",
    "            axs[0].set_ylabel('Input Token')\n",
    "            axs[0].legend()\n",
    "            axs[0].grid(True)\n",
    "\n",
    "            # Plotting residuals\n",
    "            axs[1].scatter(y_data, residuals, color='green', label='Residuals')\n",
    "            axs[1].axhline(y=0, color='r', linestyle='--', label='y=0')\n",
    "            axs[1].set_xlabel('Component in Normalized Size Vector')\n",
    "            axs[1].set_ylabel('Residual')\n",
    "            axs[1].legend()\n",
    "            axs[1].grid(True)\n",
    "\n",
    "            # Displaying the plots\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # To prevent overlap between suptitle and subplots\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def find_size_and_query_direction(model: HookedTransformer, plot_heatmaps=False, renderer=None, **kwargs) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Approximates the size direction of the model.\n",
    "    \"\"\"\n",
    "    W_pos, W_Q, W_K, W_E = model.W_pos, model.W_Q, model.W_K, model.W_E\n",
    "    d_model, d_vocab, n_ctx = model.cfg.d_model, model.cfg.d_vocab, model.cfg.n_ctx\n",
    "    assert W_pos.shape == (n_ctx, d_model), f\"W_pos.shape = {W_pos.shape} != {(n_ctx, d_model)} = (n_ctx, d_model)\"\n",
    "    assert W_Q.shape == (1, 1, d_model, d_model), f\"W_Q.shape = {W_Q.shape} != {(1, 1, d_model, d_model)} = (1, 1, d_model, d_model)\"\n",
    "    assert W_K.shape == (1, 1, d_model, d_model), f\"W_K.shape = {W_K.shape} != {(1, 1, d_model, d_model)} = (1, 1, d_model, d_model)\"\n",
    "    assert W_E.shape == (d_vocab, d_model), f\"W_E.shape = {W_E.shape} != {(d_vocab, d_model)} = (d_vocab, d_model)\"\n",
    "\n",
    "    QK = (W_E + W_pos[-1]) @ W_Q[0, 0, :, :] @ W_K[0, 0, :, :].T @ (W_E + W_pos.mean(dim=0)).T\n",
    "    assert QK.shape == (d_vocab, d_vocab), f\"QK.shape = {QK.shape} != {(d_vocab, d_vocab)} = (d_vocab, d_vocab)\"\n",
    "\n",
    "    # take SVD:\n",
    "    U, S, Vh = torch.linalg.svd(QK)\n",
    "    # adjust the free parameter of sign\n",
    "    sign = torch.sign(U[:, 0].mean())\n",
    "    U, Vh = U * sign, Vh * sign\n",
    "\n",
    "    # the size direction is the first column of Vh, normalized\n",
    "    # query direction is the first column of U, normalized\n",
    "    size_direction, query_direction = Vh[0, :], U[:, 0]\n",
    "    size_query_singular_value = S[0] * size_direction.norm() * query_direction.norm()\n",
    "    size_direction, query_direction = size_direction / size_direction.norm(), query_direction / query_direction.norm()\n",
    "\n",
    "    if plot_heatmaps:\n",
    "        size_direction_resid, query_direction_resid = size_direction @ W_E + W_pos[-1], query_direction @ W_E + W_pos.mean(dim=0)\n",
    "        size_direction_QK, query_direction_QK = size_direction_resid @ W_Q[0, 0, :, :], query_direction_resid @ W_K[0, 0, :, :]\n",
    "\n",
    "        display_size_direction_stats(size_direction, query_direction, QK, U, Vh, S,\n",
    "                                    # size_direction_resid=size_direction_resid, size_direction_QK=size_direction_QK,\n",
    "                                    # query_direction_resid=query_direction_resid, query_direction_QK=query_direction_QK,\n",
    "                                    renderer=renderer, **kwargs)\n",
    "\n",
    "    return size_direction, query_direction, size_query_singular_value.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def find_size_direction(model: HookedTransformer, **kwargs):\n",
    "    \"\"\"\n",
    "    Approximates the size direction of the model.\n",
    "    \"\"\"\n",
    "    return find_size_and_query_direction(model, **kwargs)[0]\n",
    "\n",
    "@torch.no_grad()\n",
    "def find_query_direction(model: HookedTransformer, **kwargs):\n",
    "    \"\"\"\n",
    "    Approximates the query direction of the model.\n",
    "    \"\"\"\n",
    "    return find_size_and_query_direction(model, **kwargs)[1]\n",
    "\n",
    "# %%\n",
    "@torch.no_grad()\n",
    "def find_backwards_attention(model: HookedTransformer):\n",
    "    W_pos, W_Q, W_K, W_E = model.W_pos, model.W_Q, model.W_K, model.W_E\n",
    "    d_model, d_vocab, n_ctx = model.cfg.d_model, model.cfg.d_vocab, model.cfg.n_ctx\n",
    "    assert W_pos.shape == (n_ctx, d_model), f\"W_pos.shape = {W_pos.shape} != {(n_ctx, d_model)} = (n_ctx, d_model)\"\n",
    "    assert W_Q.shape == (1, 1, d_model, d_model), f\"W_Q.shape = {W_Q.shape} != {(1, 1, d_model, d_model)} = (1, 1, d_model, d_model)\"\n",
    "    assert W_K.shape == (1, 1, d_model, d_model), f\"W_K.shape = {W_K.shape} != {(1, 1, d_model, d_model)} = (1, 1, d_model, d_model)\"\n",
    "    assert W_E.shape == (d_vocab, d_model), f\"W_E.shape = {W_E.shape} != {(d_vocab, d_model)} = (d_vocab, d_model)\"\n",
    "\n",
    "    QK = (W_E + W_pos[-1]) @ W_Q[0, 0, :, :] @ W_K[0, 0, :, :].T @ (W_E + W_pos[:, None, :]).transpose(-1, -2)\n",
    "    assert QK.shape == (n_ctx, d_vocab, d_vocab), f\"QK.shape = {QK.shape} != {(n_ctx, d_vocab, d_vocab)} = (n_ctx, d_vocab, d_vocab)\"\n",
    "    # diffs0 = QK[:, :, :-1].max(dim=0).values - QK[:, :, 1:].min(dim=0).values\n",
    "    diffs = QK[:, :, :-1] - QK[:, :, 1:].flip(dims=(0,))\n",
    "    return torch.nonzero(diffs >= 0).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title interp_max_utils\n",
    "from typing import Any, Dict, Optional, Tuple, Union\n",
    "from torchtyping import TensorType\n",
    "from enum import Enum, verify, UNIQUE, CONTINUOUS\n",
    "import enum\n",
    "import itertools\n",
    "from fancy_einsum import einsum\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformer_lens import HookedTransformer\n",
    "import transformer_lens.utils as utils\n",
    "import plotly.express as px\n",
    "import math\n",
    "\n",
    "# In[ ]:\n",
    "def complexity_of(f):\n",
    "    lines = (line.split(':') for line in f.__doc__.split('\\n'))\n",
    "    lines = (line for line in lines if line[0].lower().strip().startswith('complexity'))\n",
    "    lines = (':'.join(line[1:]).strip() if line[0].lower().strip() == 'complexity' else ':'.join(line).strip()[len('complexity'):].strip()\n",
    "             for line in lines)\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "# In[ ]:\n",
    "@torch.no_grad()\n",
    "def logit_delta_of_results(all_tokens: TensorType[\"batch\", \"n_ctx\"], predicted_logits: TensorType[\"batch\", \"d_vocab_out\"], renderer=None, histogram_all_incorrect_logit_differences: bool = False, return_summary: bool = False, hist_args={}) -> Union[float, Dict[str, Any]]: # noqa: F821\n",
    "    \"\"\"\n",
    "    Largest difference between logit(true_max) and logit(y) for y != true_max.\n",
    "    \"\"\"\n",
    "    (batch, n_ctx), (_batch, d_vocab_out) = all_tokens.shape, predicted_logits.shape\n",
    "    assert predicted_logits.shape == (batch, d_vocab_out), f\"predicted_logits.shape = {predicted_logits.shape} != {(batch, d_vocab_out)} = (batch, d_vocab_out)\"\n",
    "\n",
    "    # Extract statistics for each row\n",
    "    # Use values in all_tokens as indices to gather correct logits\n",
    "    indices_of_max = all_tokens.max(dim=-1, keepdim=True).values\n",
    "    assert indices_of_max.shape == (batch, 1), f\"indices_of_max.shape = {indices_of_max.shape} != {(batch, 1)} = (batch, 1)\"\n",
    "    correct_logits = torch.gather(predicted_logits, -1, indices_of_max)\n",
    "    assert correct_logits.shape == (batch, 1), f\"correct_logits.shape = {correct_logits.shape} != {(batch, 1)} = (batch, 1)\"\n",
    "    logits_above_correct = correct_logits - predicted_logits\n",
    "    assert logits_above_correct.shape == (batch, d_vocab_out), f\"logits_above_correct.shape = {logits_above_correct.shape} != {(batch, d_vocab_out)} = (batch, d_vocab_out)\"\n",
    "    # replace correct logit indices with large number so that they don't get picked up by the min\n",
    "    logits_above_correct[torch.arange(logits_above_correct.shape[0]), indices_of_max.squeeze()] = float('inf')\n",
    "    min_incorrect_logit = logits_above_correct.min(dim=-1).values\n",
    "    assert min_incorrect_logit.shape == (batch,), f\"min_incorrect_logit.shape = {min_incorrect_logit.shape} != {(batch,)} = (batch,)\"\n",
    "\n",
    "    if histogram_all_incorrect_logit_differences:\n",
    "        all_incorrect_logits = logits_above_correct[logits_above_correct != float('inf')]\n",
    "        summarize(all_incorrect_logits, name='all incorrect logit differences', histogram=True, hist_args=hist_args, renderer=renderer)\n",
    "\n",
    "    if return_summary:\n",
    "        return summarize(min_incorrect_logit, name='min(correct logit - incorrect logit)', renderer=renderer, histogram=True)\n",
    "\n",
    "    else:\n",
    "        return min_incorrect_logit.min().item()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "@torch.no_grad()\n",
    "def logit_delta(model: HookedTransformer, renderer=None, histogram_all_incorrect_logit_differences: bool = False, return_summary: bool = False, hist_args={}) -> Union[float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Largest difference between logit(true_max) and logit(y) for y != true_max.\n",
    "    Complexity: O(d_vocab^n_ctx * fwd_pass)\n",
    "    Complexity: fwd_pass = O(n_ctx * d_model + n_ctx * d_model + n_ctx * d_model^2 * d_hidden * 2 + n_ctx * d_hidden^2 + n_ctx * d_model^2 * d_hidden + n_ctx * d_hidden^2 * d_model + n_ctx * d_model + n_ctx * d_model^2 * d_vocab)\n",
    "    Complexity: n_ctx^2 * d_vocab * d_model^2) + (n_ctx * d_vocab * d_model^2)\n",
    "    todo fix complexity.\n",
    "    \"\"\"\n",
    "    n_ctx, d_vocab, d_vocab_out, d_model = model.cfg.n_ctx, model.cfg.d_vocab, model.cfg.d_vocab_out, model.cfg.d_model\n",
    "\n",
    "    all_tokens = compute_all_tokens(model=model)\n",
    "    assert all_tokens.shape == (d_vocab**n_ctx, n_ctx), f\"all_tokens.shape = {all_tokens.shape} != {(d_vocab**n_ctx, n_ctx)} = (d_vocab**n_ctx, n_ctx)\"\n",
    "    predicted_logits = model(all_tokens)[:,-1,:].detach().cpu()\n",
    "    assert predicted_logits.shape == (d_vocab**n_ctx, d_vocab_out), f\"predicted_logits.shape = {predicted_logits.shape} != {(d_vocab**n_ctx, d_vocab_out)} = (d_vocab**n_ctx, d_vocab_out)\"\n",
    "\n",
    "    return logit_delta_of_results(all_tokens=all_tokens, predicted_logits=predicted_logits, renderer=renderer, histogram_all_incorrect_logit_differences=histogram_all_incorrect_logit_differences, return_summary=return_summary, hist_args=hist_args)\n",
    "\n",
    "# In[ ]:\n",
    "@torch.no_grad()\n",
    "def compute_gap(all_tokens: TensorType[\"batch\", \"n_ctx\"]) -> TensorType[\"batch\"]: # noqa: F821\n",
    "    \"\"\"\n",
    "    computes the gap between the max token and the second max token in each row of all_tokens\n",
    "    \"\"\"\n",
    "    maxv = all_tokens.max(dim=-1, keepdim=True).values\n",
    "    all_but_maxv = all_tokens.clone()\n",
    "    all_but_maxv[all_but_maxv == maxv] = -all_tokens.max().item()\n",
    "    second_maxv = all_but_maxv.max(dim=-1, keepdim=True).values\n",
    "    second_maxv[second_maxv < 0] = maxv[second_maxv < 0]\n",
    "    return (maxv - second_maxv)[:, 0]\n",
    "\n",
    "# In[ ]:\n",
    "@torch.no_grad()\n",
    "def all_tokens_small_gap(model: HookedTransformer, max_min_gap: int = 1) -> TensorType[\"batch\", \"n_ctx\"]: # noqa: F821\n",
    "    \"\"\"\n",
    "    All sequences of tokens with the constraint that some token z in the sequence satisfies true_max - max_min_gap <= z < true_max\n",
    "    Complexity: O(d_vocab ^ (n_ctx - 1) * (max_min_gap * 2 + 1))\n",
    "    \"\"\"\n",
    "    n_ctx, d_vocab = model.cfg.n_ctx, model.cfg.d_vocab\n",
    "\n",
    "    all_tokens_after_start = generate_all_sequences(n_digits=d_vocab, sequence_length=n_ctx - 1)\n",
    "    all_tokens_after_start_max = all_tokens_after_start.max(dim=-1, keepdim=True).values\n",
    "    all_tokens_after_start_max_minf = all_tokens_after_start.clone()\n",
    "    all_tokens_after_start_max_minf[all_tokens_after_start_max_minf == all_tokens_after_start_max] = -max_min_gap - 1\n",
    "    all_tokens_after_start_second_max = all_tokens_after_start_max_minf.max(dim=-1, keepdim=True).values\n",
    "    first_token_max = all_tokens_after_start_max + max_min_gap + 1\n",
    "    gap_already_present = all_tokens_after_start_second_max >= all_tokens_after_start_max - max_min_gap\n",
    "    first_token_upper_min = all_tokens_after_start_max + gap_already_present.long()\n",
    "    first_token_min = torch.zeros_like(first_token_max)\n",
    "    first_token_min[~gap_already_present] = all_tokens_after_start_max[~gap_already_present] - max_min_gap\n",
    "    first_token_min[first_token_min < 0] = 0\n",
    "    first_token_max[first_token_max >= d_vocab] = d_vocab\n",
    "    first_token_upper_min[first_token_upper_min >= d_vocab] = d_vocab\n",
    "    assert first_token_max.shape == (d_vocab**(n_ctx - 1), 1), f\"first_token_max.shape = {first_token_max.shape} != {(d_vocab**(n_ctx - 1), 1)} = (d_vocab**(n_ctx - 1), 1)\"\n",
    "    assert first_token_upper_min.shape == (d_vocab**(n_ctx - 1), 1), f\"first_token_upper_min.shape = {first_token_upper_min.shape} != {(n_ctx, 1)} = (d_vocab**(n_ctx - 1), 1)\"\n",
    "    assert all_tokens_after_start_max.shape == (d_vocab**(n_ctx - 1), 1), f\"all_tokens_after_start_max.shape = {all_tokens_after_start_max.shape} != {(d_vocab**(n_ctx - 1), 1)} = (d_vocab**(n_ctx - 1), 1)\"\n",
    "    assert first_token_min.shape == (d_vocab**(n_ctx - 1), 1), f\"first_token_min.shape = {first_token_min.shape} != {(d_vocab**(n_ctx - 1), 1)} = (d_vocab**(n_ctx - 1), 1)\"\n",
    "    first_token_max, first_token_upper_min, all_tokens_after_start_max, first_token_min = first_token_max[:, 0], first_token_upper_min[:, 0], all_tokens_after_start_max[:, 0], first_token_min[:, 0]\n",
    "    first_token_ranges = [torch.cat([torch.arange(lower, mid), torch.arange(lower_big, upper)]) for lower, mid, lower_big, upper in zip(first_token_min, all_tokens_after_start_max, first_token_upper_min, first_token_max)]\n",
    "    all_tokens_with_small_gap = torch.cat([torch.cartesian_prod(first_tokens, *rest_tokens[:, None]) for first_tokens, rest_tokens in zip(first_token_ranges, all_tokens_after_start)])\n",
    "\n",
    "    return all_tokens_with_small_gap\n",
    "\n",
    "# In[ ]:\n",
    "@torch.no_grad()\n",
    "def logit_delta_small_gap_exhaustive(model: HookedTransformer, max_min_gap: int = 1, renderer=None, histogram_all_incorrect_logit_differences: bool = False, return_summary: bool = False, hist_args={}) -> Union[float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Largest difference between logit(true_max) and logit(y) for y != true_max, with the constraint that some token z in the sequence satisfies true_max - max_min_gap <= z < true_max\n",
    "    Complexity: O(d_vocab ^ (n_ctx - 1) * (max_min_gap * 2 + 1) * fwd_pass)\n",
    "    Complexity: fwd_pass = O(n_ctx * d_model + n_ctx * d_model + n_ctx * d_model^2 * d_hidden * 2 + n_ctx * d_hidden^2 + n_ctx * d_model^2 * d_hidden + n_ctx * d_hidden^2 * d_model + n_ctx * d_model + n_ctx * d_model^2 * d_vocab)\n",
    "    Complexity: n_ctx^2 * d_vocab * d_model^2) + (n_ctx * d_vocab * d_model^2)\n",
    "    todo fix complexity.\n",
    "    \"\"\"\n",
    "    n_ctx, d_vocab, d_vocab_out, d_model = model.cfg.n_ctx, model.cfg.d_vocab, model.cfg.d_vocab_out, model.cfg.d_model\n",
    "\n",
    "    all_tokens = all_tokens_small_gap(model, max_min_gap=max_min_gap)\n",
    "    assert len(all_tokens.shape) == 2 and all_tokens.shape[1] == n_ctx, f\"all_tokens.shape = {all_tokens.shape} != (_, {n_ctx}) = (_, n_ctx)\"\n",
    "    predicted_logits = model(all_tokens)[:,-1,:].detach().cpu()\n",
    "    assert len(predicted_logits.shape) == 2 and predicted_logits.shape[1] == d_vocab_out, f\"predicted_logits.shape = {predicted_logits.shape} != (_, {d_vocab_out}) = (_, d_vocab_out)\"\n",
    "\n",
    "    return logit_delta_of_results(all_tokens=all_tokens, predicted_logits=predicted_logits, renderer=renderer, histogram_all_incorrect_logit_differences=histogram_all_incorrect_logit_differences, return_summary=return_summary, hist_args=hist_args)\n",
    "\n",
    "# In[ ]:\n",
    "@torch.no_grad()\n",
    "def logit_delta_by_gap(model: HookedTransformer, renderer=None, histogram_all_incorrect_logit_differences: bool = False, return_summary: bool = False, hist_args={}) -> Dict[int, Union[float, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Largest difference between logit(true_max) and logit(y) for y != true_max, with the constraint that all non-max tokens in the sequence are strictly more than gap away from the true max, indexed by gap\n",
    "    Complexity: O(d_vocab ^ n_ctx * fwd_pass)\n",
    "    Complexity: fwd_pass = O(n_ctx * d_model + n_ctx * d_model + n_ctx * d_model^2 * d_hidden * 2 + n_ctx * d_hidden^2 + n_ctx * d_model^2 * d_hidden + n_ctx * d_hidden^2 * d_model + n_ctx * d_model + n_ctx * d_model^2 * d_vocab)\n",
    "    Complexity: n_ctx^2 * d_vocab * d_model^2) + (n_ctx * d_vocab * d_model^2)\n",
    "    todo fix complexity.\n",
    "    \"\"\"\n",
    "    n_ctx, d_vocab, d_vocab_out, d_model = model.cfg.n_ctx, model.cfg.d_vocab, model.cfg.d_vocab_out, model.cfg.d_model\n",
    "\n",
    "    all_tokens = compute_all_tokens(model=model)\n",
    "    assert all_tokens.shape == (d_vocab**n_ctx, n_ctx), f\"all_tokens.shape = {all_tokens.shape} != {(d_vocab**n_ctx, n_ctx)} = (d_vocab**n_ctx, n_ctx)\"\n",
    "    predicted_logits = model(all_tokens)[:,-1,:].detach().cpu()\n",
    "    assert predicted_logits.shape == (all_tokens.shape[0], d_vocab_out), f\"predicted_logits.shape = {predicted_logits.shape} != {(all_tokens.shape[0], d_vocab_out)} = (all_tokens.shape[0], d_vocab_out)\"\n",
    "    gaps = compute_gap(all_tokens)\n",
    "    assert gaps.shape == (all_tokens.shape[0],), f\"gaps.shape = {gaps.shape} != {(all_tokens.shape[0],)} = (all_tokens.shape[0],)\"\n",
    "    return {gap: logit_delta_of_results(all_tokens=all_tokens[gaps == gap, :], predicted_logits=predicted_logits[gaps == gap, :], renderer=renderer, histogram_all_incorrect_logit_differences=histogram_all_incorrect_logit_differences, return_summary=return_summary, hist_args=hist_args)\n",
    "            for gap in range(d_vocab)}\n",
    "\n",
    "# In[ ]:\n",
    "@torch.no_grad()\n",
    "def EU_PU(model: HookedTransformer, renderer=None, pos: int = -1) -> TensorType[\"d_vocab_q\", \"d_vocab_out\"]: # noqa: F821\n",
    "    \"\"\"\n",
    "    Calculates logits from just the EU and PU paths in position pos.\n",
    "    Complexity: O(d_vocab^2 * d_model)\n",
    "    Return shape: (d_vocab, d_vocab_out) (indexed by query token)\n",
    "    \"\"\"\n",
    "    W_E, W_pos, W_U = model.W_E, model.W_pos, model.W_U\n",
    "    d_model, n_ctx, d_vocab, d_vocab_out = model.cfg.d_model, model.cfg.n_ctx, model.cfg.d_vocab, model.cfg.d_vocab_out\n",
    "    assert W_E.shape == (d_vocab, d_model)\n",
    "    assert W_pos.shape == (n_ctx, d_model)\n",
    "    assert W_U.shape == (d_model, d_vocab_out)\n",
    "\n",
    "    result = (W_E + W_pos[pos][None, :]) @ W_U\n",
    "    assert result.shape == (d_vocab, d_vocab_out)\n",
    "\n",
    "    return result\n",
    "\n",
    "# In[ ]:\n",
    "@torch.no_grad()\n",
    "def all_attention_scores(model: HookedTransformer) -> TensorType[\"n_ctx_k\", \"d_vocab_q\", \"d_vocab_k\"]: # noqa: F821\n",
    "    \"\"\"\n",
    "    Returns pre-softmax attention of shape (n_ctx_k, d_vocab_q, d_vocab_k)\n",
    "    Complexity: O(d_vocab * d_head^2 * d_model * n_ctx)\n",
    "    \"\"\"\n",
    "    W_E, W_pos, W_Q, W_K = model.W_E, model.W_pos, model.W_Q, model.W_K\n",
    "    d_model, n_ctx, d_vocab, d_head = model.cfg.d_model, model.cfg.n_ctx, model.cfg.d_vocab, model.cfg.d_head\n",
    "    assert W_E.shape == (d_vocab, d_model)\n",
    "    assert W_pos.shape == (n_ctx, d_model)\n",
    "    assert W_Q.shape == (1, 1, d_model, d_head)\n",
    "    assert W_K.shape == (1, 1, d_model, d_head)\n",
    "\n",
    "    last_resid = (W_E + W_pos[-1]) # (d_vocab, d_model). Rows = possible residual streams.\n",
    "    assert last_resid.shape == (d_vocab, d_model), f\"last_resid.shape = {last_resid.shape} != {(d_vocab, d_model)} = (d_vocab, d_model)\"\n",
    "    key_tok_resid = (W_E + W_pos[:, None, :]) # (n_ctx, d_vocab, d_model). Dim 1 = possible residual streams.\n",
    "    assert key_tok_resid.shape == (n_ctx, d_vocab, d_model), f\"key_tok_resid.shape = {key_tok_resid.shape} != {(n_ctx, d_vocab, d_model)} = (n_ctx, d_vocab, d_model)\"\n",
    "    q = last_resid @ W_Q[0, 0, :, :] # (d_vocab, d_head).\n",
    "    assert q.shape == (d_vocab, d_head), f\"q.shape = {q.shape} != {(d_vocab, d_head)} = (d_vocab, d_head)\"\n",
    "    k = einsum('n_ctx d_vocab d_head, d_head d_model_k -> n_ctx d_model_k d_vocab', key_tok_resid, W_K[0, 0, :, :])\n",
    "    assert k.shape == (n_ctx, d_head, d_vocab), f\"k.shape = {k.shape} != {(n_ctx, d_head, d_vocab)} = (n_ctx, d_head, d_vocab)\"\n",
    "    x_scores = einsum('d_vocab_q d_head, n_ctx d_head d_vocab_k -> n_ctx d_vocab_q d_vocab_k', q, k)\n",
    "    assert x_scores.shape == (n_ctx, d_vocab, d_vocab), f\"x_scores.shape = {x_scores.shape} != {(n_ctx, d_vocab, d_vocab)} = (n_ctx, d_vocab, d_vocab)\"\n",
    "    # x_scores[pos, qt, kt] is the score from query token qt to key token kt at position pos\n",
    "\n",
    "    return x_scores\n",
    "\n",
    "# In[ ]:\n",
    "@torch.no_grad()\n",
    "def all_EVOU(model: HookedTransformer) -> TensorType[\"d_vocab\", \"d_vocab_out\"]: # noqa: F821\n",
    "    \"\"\"\n",
    "    Returns all OV results, ignoring position, of shape (d_vocab, d_vocab_out)\n",
    "    Complexity: O(d_vocab * (d_model^2 * d_head + d_head^2 * d_model + d_model^2 * d_vocab_out)) ~ O(d_vocab^2 * d_model^2)\n",
    "    \"\"\"\n",
    "    W_E, W_O, W_V, W_U = model.W_E, model.W_O, model.W_V, model.W_U\n",
    "    d_model, d_vocab, d_head, d_vocab_out = model.cfg.d_model, model.cfg.d_vocab, model.cfg.d_head, model.cfg.d_vocab_out\n",
    "    assert W_E.shape == (d_vocab, d_model)\n",
    "    assert W_O.shape == (1, 1, d_model, d_head)\n",
    "    assert W_V.shape == (1, 1, d_model, d_head)\n",
    "    assert W_U.shape == (d_model, d_vocab_out)\n",
    "\n",
    "    EVOU = W_E @ W_V[0, 0, :, :] @ W_O[0, 0, :, :] @ W_U # (d_vocab, d_vocab). EVOU[i, j] is how copying i affects j.\n",
    "    assert EVOU.shape == (d_vocab, d_vocab_out), f\"EVOU.shape = {EVOU.shape} != {(d_vocab, d_vocab_out)} = (d_vocab, d_vocab_out)\"\n",
    "    return EVOU\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "@torch.no_grad()\n",
    "def all_PVOU(model: HookedTransformer) -> TensorType[\"n_ctx\", \"d_vocab_out\"]: # noqa: F821\n",
    "    \"\"\"\n",
    "    Returns all OV results, position only, of shape (n_ctx, d_vocab_out)\n",
    "    Complexity: O(n_ctx * (d_model^2 * d_head + d_head^2 * d_model + d_model^2 * d_vocab_out)) ~ O(n_ctx * d_vocab * d_model^2)\n",
    "    \"\"\"\n",
    "    W_pos, W_O, W_V, W_U = model.W_pos, model.W_O, model.W_V, model.W_U\n",
    "    d_model, n_ctx, d_head, d_vocab_out = model.cfg.d_model, model.cfg.n_ctx, model.cfg.d_head, model.cfg.d_vocab_out\n",
    "    assert W_pos.shape == (n_ctx, d_model)\n",
    "    assert W_O.shape == (1, 1, d_model, d_head)\n",
    "    assert W_V.shape == (1, 1, d_model, d_head)\n",
    "    assert W_U.shape == (d_model, d_vocab_out)\n",
    "\n",
    "    PVOU = W_pos @ W_V[0, 0, :, :] @ W_O[0, 0, :, :] @ W_U # (n_ctx, d_vocab_out). PVOU[i, j] is how copying at position i affects logit j.\n",
    "    assert PVOU.shape == (n_ctx, d_vocab_out), f\"PVOU.shape = {PVOU.shape} != {(n_ctx, d_vocab_out)} = (n_ctx, d_vocab_out)\"\n",
    "    return PVOU\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "@torch.no_grad()\n",
    "def find_all_d_attention_scores(model: HookedTransformer, min_gap: int = 1) -> Union[TensorType[\"d_vocab_q\", \"d_vocab_k\"], TensorType[\"d_vocab_q\", \"n_ctx_max\", \"n_ctx_non_max\", \"d_vocab_k_max\", \"d_vocab_k_nonmax\"]]: # noqa: F821\n",
    "    \"\"\"\n",
    "    If input tokens are x, y, with x - y > min_gap, the minimum values of\n",
    "    score(x) - score(y).\n",
    "\n",
    "    Complexity: O(d_vocab * d_model^2 * n_ctx + d_vocab^min(3,n_ctx) * n_ctx^min(2,n_ctx-1))\n",
    "    Returns: d_attention_score indexed by\n",
    "        if n_ctx <= 2:\n",
    "            (d_vocab_q, d_vocab_k)\n",
    "        if n_ctx > 2:\n",
    "            (d_vocab_q, n_ctx_max, n_ctx_non_max, d_vocab_k_max, d_vocab_k_nonmax)\n",
    "    \"\"\"\n",
    "    n_ctx, d_vocab = model.cfg.d_model, model.cfg.n_ctx, model.cfg.d_vocab\n",
    "    x_scores = all_attention_scores(model)\n",
    "    assert x_scores.shape == (n_ctx, d_vocab, d_vocab), f\"x_scores.shape = {x_scores.shape} != {(n_ctx, d_vocab, d_vocab)} = (n_ctx, d_vocab, d_vocab)\"\n",
    "    # x_scores[pos, qt, kt] is the score from query token qt to key token kt at position pos\n",
    "\n",
    "    if n_ctx <= 2:\n",
    "        # when there are only two cases, it must be the case that either the max is in the query slot, or the non-max is in the query slot\n",
    "        scores = torch.zeros((d_vocab, d_vocab)) + float('inf')\n",
    "        for q_tok in range(d_vocab):\n",
    "            for k_tok in range(d_vocab):\n",
    "                if math.abs(k_tok - q_tok) >= min_gap:\n",
    "                    # q_tok is always in the last position\n",
    "                    scores[q_tok, k_tok] = (x_scores[0, q_tok, k_tok].item() - x_scores[-1, q_tok, q_tok].item()) * np.sign(k_tok-q_tok)\n",
    "    else:\n",
    "        # when there are more than two cases, we need to consider all cases\n",
    "        scores = torch.zeros((d_vocab, n_ctx, n_ctx, d_vocab, d_vocab)) + float('inf')\n",
    "        for q_tok in range(d_vocab):\n",
    "            for pos_of_max in range(n_ctx):\n",
    "                for k_tok_max in range(d_vocab):\n",
    "                    if pos_of_max == n_ctx - 1 and k_tok_max != q_tok: continue\n",
    "                    for pos_of_non_max in range(n_ctx):\n",
    "                        if pos_of_max == pos_of_non_max: continue\n",
    "                        for k_tok_non_max in range(k_tok_max - (min_gap - 1)):\n",
    "                            if pos_of_non_max == n_ctx - 1 and k_tok_non_max != q_tok: continue\n",
    "                            scores[q_tok, pos_of_max, pos_of_non_max, k_tok_max, k_tok_non_max] = x_scores[pos_of_max, q_tok, k_tok_max].item() - x_scores[pos_of_non_max, q_tok, k_tok_non_max].item()\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "@torch.no_grad()\n",
    "def find_min_d_attention_score(model: HookedTransformer, min_gap: int = 1, reduce_over_query=False) -> Union[float, TensorType[\"d_vocab_q\"]]: # noqa: F821\n",
    "    \"\"\"\n",
    "    If input tokens are x, y, with x - y > min_gap, the minimum value of\n",
    "    score(x) - score(y).\n",
    "\n",
    "    Complexity: O(d_vocab * d_model^2 * n_ctx + d_vocab^min(3,n_ctx) * n_ctx^min(2,n_ctx-1))\n",
    "    Returns: float if reduce_over_query else torch.Tensor[d_vocab] (indexed by query token)\n",
    "    \"\"\"\n",
    "    scores = find_all_d_attention_scores(model, min_gap=min_gap)\n",
    "    while len(scores.shape) != 1:\n",
    "        scores = scores.min(dim=-1).values\n",
    "    if reduce_over_query:\n",
    "        scores = scores.min(dim=0).values.item()\n",
    "    return scores\n",
    "\n",
    "# In[ ]:\n",
    "@torch.no_grad()\n",
    "def EU_PU_PVOU(model: HookedTransformer, attention_pattern: TensorType[\"batch\", \"n_ctx\"]) -> TensorType[\"batch\", \"d_vocab_q\", \"d_vocab_out\"]: # noqa: F821\n",
    "    \"\"\"\n",
    "    Calculates logits from EU, PU, and the positional part of the OV path for a given batch of attentions\n",
    "    attention_pattern: (batch, n_ctx) # post softmax\n",
    "    Returns: (batch, d_vocab_q, d_vocab_out)\n",
    "    Complexity: O(d_vocab^2 * d_model + d_vocab^2 * d_model^2 + batch * n_ctx * d_vocab_out + batch * d_vocab^2)\n",
    "    \"\"\"\n",
    "    n_ctx, d_vocab, d_vocab_out = model.cfg.n_ctx, model.cfg.d_vocab, model.cfg.d_vocab_out\n",
    "    batch, _ = attention_pattern.shape\n",
    "    assert attention_pattern.shape == (batch, n_ctx), f\"attention_post_softmax.shape = {attention_pattern.shape} != {(batch, n_ctx)} = (batch, n_ctx)\"\n",
    "    EUPU = EU_PU(model)\n",
    "    assert EUPU.shape == (d_vocab, d_vocab_out), f\"EUPU.shape = {EUPU.shape} != {(d_vocab, d_vocab_out)} = (d_vocab, d_vocab_out)\"\n",
    "    PVOU = all_PVOU(model)\n",
    "    assert PVOU.shape == (n_ctx, d_vocab_out), f\"PVOU.shape = {PVOU.shape} != {(n_ctx, d_vocab_out)} = (n_ctx, d_vocab_out)\"\n",
    "    PVOU_scaled = attention_pattern @ PVOU\n",
    "    assert PVOU_scaled.shape == (batch, d_vocab_out), f\"PVOU_scaled.shape = {PVOU_scaled.shape} != {(batch, d_vocab_out)} = (batch, d_vocab_out)\"\n",
    "    result = EUPU[None, :, :] + PVOU_scaled[:, None, :]\n",
    "    assert result.shape == (batch, d_vocab, d_vocab_out), f\"result.shape = {result.shape} != {(batch, d_vocab, d_vocab_out)} = (batch, d_vocab, d_vocab_out)\"\n",
    "\n",
    "    return result\n",
    "\n",
    "# In[ ]:\n",
    "# @verify(UNIQUE, CONTINUOUS)\n",
    "# class TokenType(Enum):\n",
    "#     EXACT = enum.auto() # max, or within gap\n",
    "#     BELOW_GAP = enum.auto()\n",
    "\n",
    "# In[ ]:\n",
    "@torch.no_grad()\n",
    "def worst_PVOU_gap_for(model: HookedTransformer, query_tok: int, max_tok: int,\n",
    "                       min_gap: int = 0,\n",
    "                       PVOU: Optional[TensorType[\"n_ctx\", \"d_vocab_out\"]] = None, # noqa: F821\n",
    "                       attention_score_map: Optional[TensorType[\"n_ctx_k\", \"d_vocab_q\", \"d_vocab_k\"]] = None, # noqa: F821\n",
    "                       optimize_max_query_comparison=True) -> TensorType[\"d_vocab_out\"]: # noqa: F821\n",
    "    \"\"\"\n",
    "    Returns a map of non_max_output_tok to PVOU with the worst (largest) value of PVOU[non_max_output_tok] - PVOU[max_tok],\n",
    "        across all possible attention scalings for the query token and for token values <= max_tok - min_gap.\n",
    "    Complexity: O(PVOU + attention_score_map + d_vocab_out * n_ctx^2)\n",
    "    Complexity: ~ O(n_ctx * d_vocab * d_model^2 (from PVOU) + d_vocab * d_head^2 * d_model * n_ctx (from attention_score_map) + (n_ctx * log(n_ctx) (sorting) + n_ctx^2) * d_vocab)\n",
    "    Complexity: (for n_ctx=2) O(POVU + attention_score_map + n_ctx)\n",
    "    N.B. Clever caching could reduce n_ctx^2 to n_ctx, leaving n_ctx log(n_ctx) from sorting as the dominant factor\n",
    "    N.B. If optimize_max_query_comparison is set, and n_ctx is 2, then whenever query_tok != max_tok we know exactly what the sequence is and can just compute the attention\n",
    "    \"\"\"\n",
    "    assert max_tok >= query_tok, f\"max_tok = {max_tok} < {query_tok} = query_tok\"\n",
    "    assert max_tok == query_tok or max_tok >= query_tok + min_gap, f\"max_tok = {max_tok} < {query_tok} + {min_gap} = query_tok + min_gap\"\n",
    "    n_ctx, d_vocab_out, d_vocab = model.cfg.n_ctx, model.cfg.d_vocab_out, model.cfg.d_vocab\n",
    "    if PVOU is None: PVOU = all_PVOU(model)\n",
    "    assert PVOU.shape == (n_ctx, d_vocab_out), f\"PVOU.shape = {PVOU.shape} != {(n_ctx, d_vocab_out)} = (n_ctx, d_vocab_out)\"\n",
    "    if attention_score_map is None: attention_score_map = all_attention_scores(model)\n",
    "    assert attention_score_map.shape == (n_ctx, d_vocab, d_vocab), f\"attention_scores.shape = {attention_score_map.shape} != {(n_ctx, d_vocab, d_vocab)} = (n_ctx, d_vocab, d_vocab)\"\n",
    "    worst_attention_score = torch.zeros((n_ctx,))\n",
    "    worst_attention_score[-1] = attention_score_map[-1, query_tok, query_tok]\n",
    "    if n_ctx == 2 and optimize_max_query_comparison and query_tok != max_tok:\n",
    "        worst_attention_score[0] = attention_score_map[0, query_tok, max_tok]\n",
    "        worst_PVOU = worst_attention_score.softmax(dim=-1) @ PVOU\n",
    "        return worst_PVOU - worst_PVOU[max_tok]\n",
    "    elif max_tok - min_gap < 0:\n",
    "        # everything must be the max\n",
    "        worst_PVOU = attention_score_map[:, query_tok, max_tok].softmax(dim=-1) @ PVOU\n",
    "        return worst_PVOU - worst_PVOU[max_tok]\n",
    "    else:\n",
    "        # compute the min and max attention scores for each position and query token where the key token is either max_tok or <= max_tok - gap\n",
    "        min_attention_scores_below_gap, max_attention_scores_below_gap = attention_score_map[:-1, query_tok, :max_tok+1-min_gap].min(dim=-1).values, attention_score_map[:-1, query_tok, :max_tok+1-min_gap].max(dim=-1).values\n",
    "        assert min_attention_scores_below_gap.shape == (n_ctx-1,), f\"min_attention_scores.shape = {min_attention_scores_below_gap.shape} != {(n_ctx-1,)} = (n_ctx-1,)\"\n",
    "        assert max_attention_scores_below_gap.shape == (n_ctx-1,), f\"max_attention_scores.shape = {max_attention_scores_below_gap.shape} != {(n_ctx-1,)} = (n_ctx-1,)\"\n",
    "        min_attention_scores = torch.minimum(attention_score_map[:-1, query_tok, max_tok], min_attention_scores_below_gap)\n",
    "        max_attention_scores = torch.maximum(attention_score_map[:-1, query_tok, max_tok], max_attention_scores_below_gap)\n",
    "        assert min_attention_scores.shape == (n_ctx-1,), f\"min_attention_scores.shape = {min_attention_scores.shape} != {(n_ctx-1,)} = (n_ctx-1,)\"\n",
    "        assert max_attention_scores.shape == (n_ctx-1,), f\"max_attention_scores.shape = {max_attention_scores.shape} != {(n_ctx-1,)} = (n_ctx-1,)\"\n",
    "        worst_attention_score[:-1] = min_attention_scores\n",
    "        PVOU = PVOU.T\n",
    "        assert PVOU.shape == (d_vocab_out, n_ctx), f\"PVOU.T.shape = {PVOU.shape} != {(d_vocab_out, n_ctx)} = (d_vocab_out, n_ctx)\"\n",
    "        worst_PVOU = torch.zeros((d_vocab_out, ))\n",
    "        d_PVOU = PVOU[:, :] - PVOU[max_tok, :][None, :]\n",
    "        assert d_PVOU.shape == (d_vocab_out, n_ctx), f\"d_PVOU.shape = {d_PVOU.shape} != {(d_vocab_out, n_ctx)} = (d_vocab_out, n_ctx)\"\n",
    "        # sort d_PVOU in descending order\n",
    "        _, d_PVOU_idxs = d_PVOU[:, :-1].sort(dim=-1, descending=True)\n",
    "        for non_max_output_tok in range(d_vocab_out):\n",
    "            worst_attention_score[:-1] = min_attention_scores\n",
    "            for i in d_PVOU_idxs[non_max_output_tok, :]:\n",
    "                # compare d_PVOU weighted by softmax of worst_attention_score for worst_attention_score[i] in (min_attention_scores[i], max_attention_scores[i])\n",
    "                # set worst_attention_score[i] to whichever one is worse (more positive)\n",
    "                # print(d_PVOU.shape, worst_attention_score.softmax(dim=-1).shape)\n",
    "                min_d_PVOU = worst_attention_score.softmax(dim=-1) @ d_PVOU[non_max_output_tok, :]\n",
    "                worst_attention_score[i] = max_attention_scores[i]\n",
    "                max_d_PVOU = worst_attention_score.softmax(dim=-1) @ d_PVOU[non_max_output_tok, :]\n",
    "                if min_d_PVOU > max_d_PVOU: worst_attention_score[i] = min_attention_scores[i]\n",
    "            worst_PVOU[non_max_output_tok] = worst_attention_score.softmax(dim=-1) @ d_PVOU[non_max_output_tok, :]\n",
    "            # print(i, min_attention_scores[i], worst_attention_score[i], max_attention_scores[i], min_d_PVOU, max_d_PVOU, d_PVOU[i])\n",
    "        # return the PVOU for the worst_attention_score\n",
    "        return worst_PVOU\n",
    "\n",
    "# In[ ]:\n",
    "@torch.no_grad()\n",
    "def all_worst_PVOU(model: HookedTransformer, min_gap: int = 0, tqdm=None, **kwargs) -> TensorType[\"d_vocab_q\", \"d_vocab_max\", \"d_vocab_out\"]: # noqa: F821\n",
    "    \"\"\"\n",
    "    Returns the mixture of PVOUs with the worst (largest) value of PVOU[non_max_output_tok] - PVOU[max_tok], across all possible attention scalings for the query token and for token values <= max_tok - min_gap.\n",
    "    Complexity: O(PVOU + attention_score_map + n_ctx^2 * d_vocab^3)\n",
    "    Complexity: ~ O(n_ctx * d_vocab * d_model^2 (from PVOU) + d_vocab * d_head^2 * d_model * n_ctx (from attention_score_map) + (n_ctx * log(n_ctx) (sorting) + n_ctx^2) * d_vocab^3)\n",
    "    Complexity: (for n_ctx=2) O(PVOU + attention_score_map + n_ctx * d_vocab^2)\n",
    "    N.B. Clever caching could reduce n_ctx^2 to n_ctx, leaving n_ctx log(n_ctx) * d_vocab^3 from sorting as the dominant factor.\n",
    "    N.B. for max_of_{two,three}, this is maybe? worse than exhaustive enumeration (oops)\n",
    "    \"\"\"\n",
    "    local_tqdm = make_local_tqdm(tqdm)\n",
    "    n_ctx, d_vocab_out, d_vocab = model.cfg.n_ctx, model.cfg.d_vocab_out, model.cfg.d_vocab\n",
    "    PVOU = all_PVOU(model)\n",
    "    assert PVOU.shape == (n_ctx, d_vocab_out), f\"PVOU.shape = {PVOU.shape} != {(n_ctx, d_vocab_out)} = (n_ctx, d_vocab_out)\"\n",
    "    attention_score_map = all_attention_scores(model)\n",
    "    assert attention_score_map.shape == (n_ctx, d_vocab, d_vocab), f\"attention_scores.shape = {attention_score_map.shape} != {(n_ctx, d_vocab, d_vocab)} = (n_ctx, d_vocab, d_vocab)\"\n",
    "    result = torch.zeros((d_vocab, d_vocab, d_vocab_out)) + float('nan')\n",
    "    for query_tok in local_tqdm(range(d_vocab), total=d_vocab):\n",
    "        for max_tok in [query_tok] + list(range(query_tok+np.max([1, min_gap]), d_vocab)):\n",
    "            result[query_tok, max_tok, :] = worst_PVOU_gap_for(model, query_tok, max_tok, min_gap=min_gap, PVOU=PVOU, attention_score_map=attention_score_map, **kwargs)\n",
    "\n",
    "    return result\n",
    "\n",
    "# In[ ]:\n",
    "@torch.no_grad()\n",
    "def worst_EVOU_gap_for(model: HookedTransformer, query_tok: int, max_tok: int,\n",
    "                       min_gap: int = 0,\n",
    "                       EVOU: Optional[TensorType[\"d_vocab\", \"d_vocab_out\"]] = None, # noqa: F821\n",
    "                       attention_score_map: Optional[TensorType[\"n_ctx_k\", \"d_vocab_q\", \"d_vocab_k\"]] = None, # noqa: F821\n",
    "                       optimize_max_query_comparison=True) -> TensorType[\"d_vocab_out\"]: # noqa: F821\n",
    "    \"\"\"\n",
    "    Returns the map of non_max_output_tok to worst (largest) value of EVOU[non_max_output_tok] - EVOU[max_tok], across all possible attention scalings for the query token\n",
    "        and for token values <= max_tok - min_gap.\n",
    "    To deal with the fact that attention and EVOU are not truly independent, we relax the \"worst\" calculation by saying that the attention paid to a given token in a given position\n",
    "        is the min of (most attention paid to this token in this position) and (most attention paid to any token < max in this position).\n",
    "    \"<\" is relaxed to \"<=\" when the token under consideration is the max token.\n",
    "\n",
    "    Complexity: O(EVOU + attention_score_map + n_ctx * d_vocab + d_vocab^2)\n",
    "    Complexity: (for n_ctx=2) O(EOVU + attention_score_map + d_vocab + n_ctx)\n",
    "    #N.B. If optimize_max_query_comparison is set, and n_ctx is 2, then whenever query_tok != max_tok we know exactly what the sequence is and can just compute the attention\n",
    "    \"\"\"\n",
    "    assert max_tok >= query_tok, f\"max_tok = {max_tok} < {query_tok} = query_tok\"\n",
    "    assert max_tok == query_tok or max_tok >= query_tok + min_gap, f\"max_tok = {max_tok} < {query_tok} + {min_gap} = query_tok + min_gap\"\n",
    "    n_ctx, d_vocab_out, d_vocab = model.cfg.n_ctx, model.cfg.d_vocab_out, model.cfg.d_vocab\n",
    "    if EVOU is None: EVOU = all_EVOU(model)\n",
    "    assert EVOU.shape == (d_vocab, d_vocab_out), f\"EVOU.shape = {EVOU.shape} != {(d_vocab, d_vocab_out)} = (d_vocab, d_vocab_out)\"\n",
    "    if attention_score_map is None: attention_score_map = all_attention_scores(model)\n",
    "    assert attention_score_map.shape == (n_ctx, d_vocab, d_vocab), f\"attention_scores.shape = {attention_score_map.shape} != {(n_ctx, d_vocab, d_vocab)} = (n_ctx, d_vocab, d_vocab)\"\n",
    "    if n_ctx == 2 and optimize_max_query_comparison and query_tok != max_tok:\n",
    "        worst_attention_score = torch.zeros((n_ctx,))\n",
    "        worst_attention_score[-1] = attention_score_map[-1, query_tok, query_tok]\n",
    "        worst_attention_score[0] = attention_score_map[0, query_tok, max_tok]\n",
    "        worst_EVOU = worst_attention_score.softmax(dim=-1) @ EVOU[torch.tensor([max_tok, query_tok]), :]\n",
    "        return worst_EVOU - worst_EVOU[max_tok]\n",
    "    elif max_tok - min_gap < 0:\n",
    "        # everything must be the max\n",
    "        assert max_tok == query_tok, f\"max_tok = {max_tok} != {query_tok} = query_tok\"\n",
    "        worst_EVOU = EVOU[max_tok, :]\n",
    "        return worst_EVOU - worst_EVOU[max_tok]\n",
    "    else:\n",
    "        # for each non-query position, compute the min and max attention scores for that position and query token where the key token is < max_tok, and also when the key token is <= max_tok\n",
    "        max_nonmax_tok = np.min([max_tok - 1, max_tok - min_gap])\n",
    "        min_attention_scores_without_max, max_attention_scores_without_max = attention_score_map[:-1, query_tok, :max_nonmax_tok+1].min(dim=-1).values, attention_score_map[:-1, query_tok, :max_nonmax_tok+1].max(dim=-1).values\n",
    "        assert min_attention_scores_without_max.shape == (n_ctx-1,), f\"min_attention_scores_without_max.shape = {min_attention_scores_without_max.shape} != {(n_ctx-1,)} = (n_ctx-1,)\"\n",
    "        assert max_attention_scores_without_max.shape == (n_ctx-1,), f\"max_attention_scores_without_max.shape = {max_attention_scores_without_max.shape} != {(n_ctx-1,)} = (n_ctx-1,)\"\n",
    "        # for each key token below the max, compute the min and max attention scores for that token and query token where the key token is <= max_tok\n",
    "        # if query token is max, we assume all other tokens are the same; otherwise, we pick the minimal attention slot for the max token and the other slots for the non-max, except when we consider all maxes but the query\n",
    "        # we must subtract off the maximum to avoid overflow, as per https://github.com/pytorch/pytorch/blob/bc047ec906d8e1730e2ccd8192cef3c3467d75d1/aten/src/ATen/native/cpu/SoftMaxKernel.cpp#L115-L136\n",
    "        attention_to_query = attention_score_map[-1, query_tok, query_tok]\n",
    "        attentions_to_max = attention_score_map[:-1, query_tok, max_tok]\n",
    "        attention_offset = torch.maximum(attentions_to_max.max(), attention_to_query)\n",
    "        attention_to_max_exp = (attentions_to_max - attention_offset).exp().sum()\n",
    "        attention_to_query_exp = (attention_to_query - attention_offset).exp()\n",
    "        attention_sum = attention_to_max_exp + attention_to_query_exp\n",
    "        EVOUs = torch.zeros((max_tok+1, d_vocab_out))\n",
    "        EVOUs[max_tok, :] = EVOU[max_tok, :] * attention_to_max_exp / attention_sum + EVOU[query_tok, :] * attention_to_query_exp / attention_sum\n",
    "        assert EVOUs[max_tok, :].shape == (d_vocab_out,), f\"EVOU_all_maxes.shape = {EVOUs[max_tok, :].shape} != {(d_vocab_out,)} = (d_vocab_out,)\"\n",
    "\n",
    "        # consider all tokens < max, compute EVOU for each\n",
    "        attention_to_max = attention_score_map[:-1, query_tok, max_tok].min()\n",
    "        for non_max_tok in range(max_nonmax_tok+1):\n",
    "            # we need to relax attention to non-max, picking the attention to this slot from the min of largest attention to this token and largest attention to this slot\n",
    "            max_attention_to_non_max = attention_score_map[:-1, query_tok, non_max_tok].max()\n",
    "            attention_to_non_max = torch.minimum(max_attention_to_non_max, max_attention_scores_without_max)\n",
    "            if query_tok == max_tok:\n",
    "                # we must subtract off the maximum to avoid overflow, as per https://github.com/pytorch/pytorch/blob/bc047ec906d8e1730e2ccd8192cef3c3467d75d1/aten/src/ATen/native/cpu/SoftMaxKernel.cpp#L115-L136\n",
    "                attention_offset = torch.maximum(attention_to_query, attention_to_non_max.max())\n",
    "                attention_to_max_exp = (attention_to_max - attention_offset).exp()\n",
    "                attention_to_query_exp = (attention_to_query - attention_offset).exp()\n",
    "                attention_to_non_max_exp = (attention_to_non_max - attention_offset).exp().sum()\n",
    "                attention_sum = attention_to_non_max_exp + attention_to_query_exp\n",
    "                EVOUs[non_max_tok, :] = EVOU[non_max_tok, :] * attention_to_non_max_exp / attention_sum + EVOU[query_tok, :] * attention_to_query_exp / attention_sum\n",
    "            else:\n",
    "                # we must subtract off the maximum to avoid overflow, as per https://github.com/pytorch/pytorch/blob/bc047ec906d8e1730e2ccd8192cef3c3467d75d1/aten/src/ATen/native/cpu/SoftMaxKernel.cpp#L115-L136\n",
    "                attention_offset = torch.maximum(torch.maximum(attention_to_max, attention_to_query), attention_to_non_max.max())\n",
    "                attention_to_non_max_exp = (attention_to_non_max - attention_offset).exp()\n",
    "                # drop the smallest value in attention_to_non_max\n",
    "                attention_to_non_max_exp = attention_to_non_max_exp.sum() - attention_to_non_max_exp.min()\n",
    "                attention_to_max_exp = (attention_to_max - attention_offset).exp()\n",
    "                attention_to_query_exp = (attention_to_query - attention_offset).exp()\n",
    "                attention_sum = attention_to_non_max_exp + attention_to_query_exp + attention_to_max_exp\n",
    "                EVOUs[non_max_tok, :] = EVOU[non_max_tok, :] * attention_to_non_max_exp / attention_sum + EVOU[query_tok, :] * attention_to_query_exp / attention_sum + EVOU[max_tok, :] * attention_to_max_exp / attention_sum\n",
    "        # subtract off the max_tok EVOU\n",
    "        print(EVOUs)\n",
    "        EVOUs = EVOUs - EVOUs[:, max_tok][:, None]\n",
    "        # return the worst EVOU\n",
    "        return EVOUs.max(dim=0).values\n",
    "# print(worst_EVOU_gap_for(model, 63, 63, 2))\n",
    "# In[ ]:\n",
    "@torch.no_grad()\n",
    "def all_worst_EVOU(model: HookedTransformer, min_gap: int = 0, tqdm=None, **kwargs) -> TensorType[\"d_vocab_q\", \"d_vocab_max\", \"d_vocab_out\"]: # noqa: F821\n",
    "    \"\"\"\n",
    "    Returns the mixture of EVOUs with the worst (largest) value of EVOU[non_max_output_tok] - EVOU[max_tok], across all possible attention scalings for the query token and for token values <= max_tok - min_gap.\n",
    "    Complexity: O(EVOU + attention_score_map + (n_ctx + d_vocab) * d_vocab^3)\n",
    "    Complexity: (for n_ctx=2) O(EVOU + attention_score_map + (n_ctx + d_vocab) * d_vocab^2)\n",
    "    N.B. for max_of_{two,three}, this is maybe? worse than exhaustive enumeration (oops)\n",
    "    \"\"\"\n",
    "    local_tqdm = make_local_tqdm(tqdm)\n",
    "    n_ctx, d_vocab_out, d_vocab = model.cfg.n_ctx, model.cfg.d_vocab_out, model.cfg.d_vocab\n",
    "    EVOU = all_EVOU(model)\n",
    "    assert EVOU.shape == (d_vocab, d_vocab_out), f\"EVOU.shape = {EVOU.shape} != {(d_vocab, d_vocab_out)} = (d_vocab, d_vocab_out)\"\n",
    "    attention_score_map = all_attention_scores(model)\n",
    "    assert attention_score_map.shape == (n_ctx, d_vocab, d_vocab), f\"attention_scores.shape = {attention_score_map.shape} != {(n_ctx, d_vocab, d_vocab)} = (n_ctx, d_vocab, d_vocab)\"\n",
    "    result = torch.zeros((d_vocab, d_vocab, d_vocab_out)) + float('nan')\n",
    "    for query_tok in local_tqdm(range(d_vocab), total=d_vocab):\n",
    "        for max_tok in [query_tok] + list(range(query_tok+np.max([1, min_gap]), d_vocab)):\n",
    "            result[query_tok, max_tok, :] = worst_EVOU_gap_for(model, query_tok, max_tok, min_gap=min_gap, EVOU=EVOU, attention_score_map=attention_score_map, **kwargs)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title imports\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = max_of_2.get_model(train_if_necessary=False)\n",
    "model = max_of_2.get_model(train_if_necessary=False)\n",
    "all_tokens = compute_all_tokens(model)\n",
    "all_logits = model(all_tokens)\n",
    "expected_max = all_tokens.max(dim=-1).values\n",
    "predicted_max = all_logits[..., -1, :].argmax(dim=-1)\n",
    "print(f\"Model Accuracy: {acc_fn(all_logits, all_tokens, return_per_token=False) * 100}%\")\n",
    "print(f\"Number Incorrect Sequences: {(predicted_max != expected_max).sum()}\")\n",
    "print(f\"Model Loss: {loss_fn(all_logits, all_tokens, return_per_token=False)}\")\n",
    "print(f\"{all_logits.dtype} ULP on log-softmax = ULP at 1.0 = -(exp(0) - eps).log() = {torch.finfo(all_logits.dtype).eps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the loss is *lower* than what we would expect from a one ULP (unit of least precision) error in the log-softmax calculation at the end.  This is about as good as the model can possibly do with 32-bit floats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Size Direction\n",
    "\n",
    "We can run SVD on EQKE (actually $(W_E + W_{\\text{pos}}[-1]) W_Q W_K^T \\left(W_E + W_{\\text{pos}}\\text{.mean}(\\text{dim}=0)\\right)^T$) to find the size direction and the query direction in token space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_direction, query_direction, size_query_singular_value = find_size_and_query_direction(model, plot_heatmaps=True, colorscale='Picnic_r')\n",
    "print(f\"Size direction: {size_direction}\\nQuery direction: {query_direction}\\nSingular value: {size_query_singular_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A couple of notes:\n",
    "- SVD is only unique up to the sign of each singular vector.  PyTorch SVD gives us a negative query direction vector, so we negate both the query and size direction vectors.\n",
    "- If we fit the size direction to a cubic (or quintic), the bounds on the errors might not actually give us enough information to ensure adjacent tokens are ordered correctly.  But if we fit the differences in size-direction overlap of adjacent tokens to a quadratic (or quartic), we see that all differences are positive, and so we can get monotonicity even with worst-case errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation and relevance\n",
    "- The first singular value is just over 8,000; the next singular value is just under 30, so to a first approximation there's only one thing going on.\n",
    "- However, the remainder of the QK circuit (labeled \"Residual\" on the \"Contribution of the first singular value to Attention\" plot) is not actually small enough to neglect in all cases.\n",
    "  - Looking at the \"Size Direction Δ & Fit\" plots, consider the \"resid.max - resid.min (worst-case independent query)\" line.  This line results from taking the remainder of the QK circuit, finding the maximum possible difference in attention, and scaling it according to the worst possible query token (the one which overlaps the least with the size direction).  The position of this line shows that the size direction is enough to explain the majority of cases (many sequences with ($i$, $i+1$) will have a comparatively large attention gap just based on the size direction, and most sequences with larger gaps will certainly pay more attention to the larger token), but not all of them.\n",
    "  - Looking at the \"(resid[i] - resid[i+1]).max (worst-case query)\" line suggests that even accounting for exact attention values, the model might pay more attention to the smaller token!  Let's compute if this ever happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The model pays more attention to the smaller token for the following seuqences:')\n",
    "for minpos, qtok, ktokmin in find_backwards_attention(model):\n",
    "    if qtok not in range(ktokmin, ktokmin+2): descr = \"(invalid! query token not in sequence)\"\n",
    "    elif (minpos == 1 and qtok != ktokmin): descr = \"(invalid! minimum in the query position but not equal to query token)\"\n",
    "    elif (minpos == 0 and qtok != ktokmin+1): descr = \"(invalid! contradictory minimum position and query token constraints)\"\n",
    "    else: descr = \"(valid!)\"\n",
    "    print(f\"Tokens: {ktokmin}, {ktokmin+1};\\tQuery: {qtok};\\tPosition of the Minimum: {minpos}\\t{descr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably, the model only pays more attention to smaller tokens when the query token is not present in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compact Guarantees\n",
    "- If we are trying to generate the most compact guarantee, neither the SVD nor the fit buy us much.  There are two locations in the proof where we might hope to gain in compactness by using the size direction:\n",
    "  1. In explaining the behavior of the QK circuit.  But in generating a guarantee, we still have to establish that the particular QK circuit is doing the right thing, and I'm not sure how to compactly argue that the principle component of a product of matrices is what it is without multiplying out the matrices.  But if we multiply out the matrices, we have all of the pairwise attention weights, and so we don't need the size direction to explain the behavior of the QK circuit.\n",
    "  2. In using the behavior of the QK circuit to explain the rest of the transformer.  Here in fact we get some benefit from having a compact description of *what* the QK circuit is doing.  Here we get a lot of benefit from some simple cut-off behavior (computing, for example, the minimal attention gap between tokens separated by at least two), but further dependencies between the QK circuit and the rest of the transformer seem to be more about the query direction than the size direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis**: In almost all cases (for almost all possible sequences), either:\n",
    "1. The best reasoning we can do with the size direction isn't enough to get us 100% accuracy, and the loss will be rather sensitive to the exact attention values; or\n",
    "2. A simple lower bound on the attention gap between non-adjacent tokens is enough to get us 100% accuracy, and the loss will be so insensitive to the exact attention values that we won't get much benefit from any approximation more detailed than a lower bound.\n",
    "\n",
    "To test this hypothesis, we can compute, for each maximum token, how much attention needs to be on that token in order for the model to predict the correct output.\n",
    "\n",
    "What does \"needs to be\" mean, though?\n",
    "It could mean:\n",
    "1. Given exactly how the rest of the transformer behaves, what's the cutoff for attention?\n",
    "2. For some particular functional model of the rest of the transformer's behavior and bounds on the errors, what's the cutoff for attention?\n",
    "\n",
    "or anything in between.\n",
    "\n",
    "Let's compute the histogram of attention cutoffs for a variety of interpretations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ctx, d_vocab, d_vocab_out = model.cfg.n_ctx, model.cfg.d_vocab, model.cfg.d_vocab_out\n",
    "EUPU: Float[Tensor, \"d_vocab d_vocab_out\"] = EU_PU(model, pos=-1)\n",
    "#assert EUPU.shape == (d_vocab, d_vocab_out), f\"EUPU.shape = {EUPU.shape} != {(d_vocab, d_vocab_out)} = (d_vocab, d_vocab_out)\"\n",
    "EVOU: Float[Tensor, \"d_vocab d_vocab_out\"] = all_EVOU(model)\n",
    "#assert EVOU.shape == (d_vocab, d_vocab_out), f\"EVOU.shape = {EVOU.shape} != {(d_vocab, d_vocab_out)} = (d_vocab, d_vocab_out)\"\n",
    "PVOU: Float[Tensor, \"n_ctx d_vocab_out\"] = all_PVOU(model)\n",
    "#assert PVOU.shape == (n_ctx, d_vocab_out), f\"PVOU.shape = {PVOU.shape} != {(n_ctx, d_vocab_out)} = (n_ctx, d_vocab_out)\"\n",
    "# assume we vary only the attention, find the minimum attention required for the correct output\n",
    "all_tokens: Float[Tensor, \"batch n_ctx\"] = compute_all_tokens(model)\n",
    "all_tokens_max: Float[Tensor, \"batch\"] = all_tokens.max(dim=-1).values\n",
    "all_tokens_max_pos: Float[Tensor, \"batch\"] = all_tokens.argmax(dim=-1)\n",
    "all_tokens_EUPU: Float[Tensor, \"batch d_vocab_out\"] = EUPU[all_tokens[:, -1], :]\n",
    "all_tokens_EVOU_PVOU: Float[Tensor, \"batch n_ctx d_vocab_out\"] = EVOU[all_tokens, :] + PVOU\n",
    "# to compute the minimum post-softmax attention required for the correct output, we center each output so the logit of the correct output is 0\n",
    "# then we want to compute the minimum p such that p * (logit of incorrect output on max token) + (1 - p) * (logit of incorrect token on non-max token) + (logit of incorrect output on residual path) < 0\n",
    "# or equivalently p * (logit of incorrect output on max token - logit of incorrect output on non-max token) < -(logit of incorrect output on residual path + logit of incorrect token on non-max token)\n",
    "# or p * sign(logit of incorrect output on max token - logit of incorrect output on non-max token) < -(logit of incorrect output on residual path + logit of incorrect token on non-max token) / abs(logit of incorrect output on max token - logit of incorrect output on non-max token)\n",
    "# if the logit of the incorrect output on EUPU + the max token is positive, we say nan (no attention is enough)\n",
    "# if the logit of the incorrect output on the non-max token is negative, we say 0 (any amount of attention is enough)\n",
    "all_tokens_EUPU -= all_tokens_EUPU[torch.arange(all_tokens_EUPU.shape[0]), all_tokens_max][:, None]\n",
    "for b in range(all_tokens_EVOU_PVOU.shape[0]):\n",
    "    for n in range(all_tokens_EVOU_PVOU.shape[1]):\n",
    "        all_tokens_EVOU_PVOU[b, n, :] = all_tokens_EVOU_PVOU[b, n, :] - all_tokens_EVOU_PVOU[b, n, all_tokens_max[b]]\n",
    "\n",
    "# fold all_tokens_EUPU into all_tokens_EVOU_PVOU\n",
    "all_tokens_EVOU_PVOU_max_token: Float[Tensor, \"batch d_vocab_out\"] = all_tokens_EVOU_PVOU[torch.arange(all_tokens_EVOU_PVOU.shape[0]), all_tokens_max_pos, :] + all_tokens_EUPU\n",
    "# we're worst off when the attention on the non-max token is largest, so we take max\n",
    "all_tokens_EVOU_PVOU_non_max_token_tmp = all_tokens_EVOU_PVOU.clone()\n",
    "all_tokens_EVOU_PVOU_non_max_token_tmp[torch.arange(all_tokens_EVOU_PVOU.shape[0]), all_tokens_max_pos, :] = -float('inf')\n",
    "all_tokens_EVOU_PVOU_non_max_token: Float[Tensor, \"batch d_vocab_out\"] = all_tokens_EVOU_PVOU_non_max_token_tmp.max(dim=-2).values + all_tokens_EUPU\n",
    "attention_never_enough = (all_tokens_EVOU_PVOU_max_token > 0)\n",
    "logit_diff = all_tokens_EVOU_PVOU_max_token - all_tokens_EVOU_PVOU_non_max_token\n",
    "logit_gap = -all_tokens_EVOU_PVOU_non_max_token / logit_diff.abs()\n",
    "# where logit_diff < 0, we want smallest p > -logit_gap\n",
    "# where logit_diff > 0 we want smallest p < logit_gap\n",
    "min_p_1 = -logit_gap[logit_diff < 0]\n",
    "min_p_1 = torch.max(min_p_1, torch.zeros_like(min_p_1))\n",
    "min_p_2 = logit_gap[logit_diff > 0]\n",
    "min_p_2 = torch.min(min_p_2, torch.zeros_like(min_p_2))\n",
    "min_p = torch.cat([min_p_1, min_p_2], dim=0)\n",
    "#from analysis_utils import hist\n",
    "hist(min_p)\n",
    "hist(min_p[min_p != 0])\n",
    "hist(min_p[min_p < 0])\n",
    "# find pre-softmax\n",
    "# min_p = weight on max = e^max / (e^max + e^non-max) = 1 / (1 + e^(non-max - max)\n",
    "# e^(non_max - max) = 1 / min_p - 1\n",
    "# non_max - max = log(1 / min_p - 1)\n",
    "# max - non_max = -log(1 / min_p - 1)\n",
    "min_attn = -(1 / min_p[min_p > 0] - 1).log()\n",
    "hist(min_attn)\n",
    "hist(min_attn[min_attn > 0])\n",
    "\n",
    "# all_tokens_EVOU_PVOU_max_token - all_tokens_EVOU_PVOU_non_max_token\n",
    "# -all_tokens_EVOU_PVOU_non_max_token / (all_tokens_EVOU_PVOU_max_token - all_tokens_EVOU_PVOU_non_max_token).abs()\n",
    "\n",
    "# print(all_tokens_EUPU.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HERE\n",
    "\n",
    "There are numerous approximations to this computation we might want to consider:\n",
    "1. For every sequence, how much attention needs to be on the correct token for predicting the correct output?\n",
    "2. For each max token, for each position it could be in, for the worst-case (or average-case) query token compatible with that choice (determining the residual stream impact), for the worst-case (or average-case) non-max-token OV behavior (per-logit), how much attention needs to be on the max token for the max token logit to be higher than the logit corresponding to the other token?  (Here we'd also need to verify that the OV behavior has all diagonal entries higher than all off-diagonal entries in the same row, to ensure that we can find the worst case for each logit separately.)\n",
    "3. For each max token, we can compute the smallest (or average) gap between the logit of that token and the logit of any other token, and then reduce across positions.  Then we can compute the largest (or average) gap between logits in the residual stream impact, and the largest (or average) gap between logits across all other copying behavior.  Finally we can compute how much attention needs to be on the max token to prevent the worst-case (or average) behavior of the rest of the transformer from outweighing the correct behavior on the max token.\n",
    "\n",
    "Although there are even more approximations we might consider, let's zoom out to make sense of the landscape here.  When picking an approximation to compute, we should ask: \"What interpretation are we validating with this computation?\"\n",
    "\n",
    "1. The first approximation validates the hypothesis \"the model (somehow) pays enough more attention to the largest token, where 'enough' means: enough that whatever other computations are going on, the model outputs the correct answer\".  Notably, this doesn't explain very much.\n",
    "2. The second approximation validates the hypothesis \"for every sequence, find the maximum, and consider all sequences with the same maximum in the same position; whatever computation is going on in the rest of the model, it's sufficiently invariant over the details of the rest of the sequence that the model outputs the correct answer\".\n",
    "3. The third approximation validates the hypothesis \"the behavior on the non-max token and the bahvior on the skip-connection is irrelevant noise; we pay enough more attention to the max token that we can neglect everything else that happens beyond getting a simple bound on how much it perturbs the output\".\n",
    "\n",
    "Notably, the computation for 3 is (slightly) more compact than the computation for 2 (which itself is slightly more compact than the computation for 1), which, to our eyes at least, is minor evidence in favor of compact proofs being a good proxy for human interpretability.\n",
    "\n",
    "However, in the max-of-2 model, exhaustive enumeration, which is exponential in the sequence length, is indistinguishable from a pairwise quadratic analysis (quadratic and exponential are the same when the exponent is 2).  There are not many asymptotic gains to be had here, and compactness differences are somewhat harder to see.\n",
    "\n",
    "Let's plot how much attention is given to the max token in each sequence, and how much attention is required, according to each of these three computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_attention_to_max(model: HookedTransformer) -> torch.Tensor:\n",
    "#     \"\"\"Compute the attention given to the max token in each sequence.\"\"\"\n",
    "#     n_ctx, d_vocab = model.cfg.n_ctx, model.cfg.d_vocab\n",
    "#     # pre soft-max\n",
    "#     all_attention_scores = all_attention_scores(model)\n",
    "#     assert all_attention_scores.shape == (n_ctx, d_vocab, d_vocab), f\"all_attention.shape = {all_attention_scores.shape} != ({n_ctx}, {d_vocab}, {d_vocab}) = (n_ctx_k, d_vocab_k, d_vocab_q)\"\n",
    "\n",
    "#     # compute soft-max\n",
    "\n",
    "\n",
    "\n",
    "#     all_tokens = compute_all_tokens(model)\n",
    "#     all_logits = model(all_tokens)\n",
    "#     expected_max = all_tokens.max(dim=-1).values\n",
    "#     predicted_max = all_logits[..., -1, :].argmax(dim=-1)\n",
    "#     return (all_logits.gather(-1, expected_max.unsqueeze(-1)) * (predicted_max == expected_max).float()).sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  HERE\n",
    " # %%\n",
    " # compute EU PU\n",
    " W_E, W_pos, W_U = model.W_E, model.W_pos, model.W_U\n",
    " print(W_E.shape, W_pos.shape, W_U.shape)\n",
    " line(W_pos[-1] @ W_U)\n",
    " imshow(W_E @ W_U)\n",
    " imshow((W_E + W_pos[-1]) @ W_U)\n",
    " analyze_svd(W_E @ W_U)\n",
    " analyze_svd((W_E + W_pos[-1]) @ W_U)\n",
    " # %%\n",
    " # compute OV\n",
    " import analysis_utils\n",
    " analysis_utils.calculate_OV_of_pos_embed(model)\n",
    " analysis_utils.calculate_copying(model)\n",
    " # %%\n",
    " W_E, W_pos, W_U, W_V, W_O = model.W_E, model.W_pos, model.W_U, model.W_V, model.W_O\n",
    " analyze_svd(W_E @ W_V[0, 0] @ W_O[0, 0] @ W_U)\n",
    " # %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import analysis_utils\n",
    "# reload analysis_utils module\n",
    "#import importlib\n",
    "#importlib.reload(analysis_utils)\n",
    "#analysis_utils.analyze_EVOU(model, scale_by_singular_value=False)\n",
    "#analysis_utils.analyze_PVOU(model)\n",
    "#analysis_utils.analyze_PU(model)\n",
    "#analysis_utils.analyze_EU(model)\n",
    "analyze_EVOU(model, scale_by_singular_value=False)\n",
    "analyze_PVOU(model)\n",
    "analyze_PU(model)\n",
    "analyze_EU(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
